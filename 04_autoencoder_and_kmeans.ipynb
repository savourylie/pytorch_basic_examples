{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mnist\n",
    "\n",
    "data_folder  = '../../data/mnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading train-images-idx3-ubyte.gz...\n",
      "Downloading t10k-images-idx3-ubyte.gz...\n",
      "Downloading train-labels-idx1-ubyte.gz...\n",
      "Downloading t10k-labels-idx1-ubyte.gz...\n",
      "Download complete.\n",
      "Save complete.\n"
     ]
    }
   ],
   "source": [
    "# from glob import glob\n",
    "# import os\n",
    "\n",
    "# data_train = pd.read_csv(data_folder + 'train.csv')\n",
    "\n",
    "mnist.init()\n",
    "X_train, y_train, X_test, y_test = mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.min(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train_mnist.pkl', 'wb') as handle:\n",
    "    pickle.dump(X_train, handle)\n",
    "with open('X_test_mnist.pkl', 'wb') as handle:\n",
    "    pickle.dump(X_test, handle)\n",
    "with open('y_train_mnist.pkl', 'wb') as handle:\n",
    "    pickle.dump(y_train, handle)\n",
    "with open('y_test_mnist.pkl', 'wb') as handle:\n",
    "    pickle.dump(y_test, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train_mnist.pkl', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "with open('X_test_mnist.pkl', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)\n",
    "with open('y_train_mnist.pkl', 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "with open('y_test_mnist.pkl', 'rb') as handle:\n",
    "    y_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, label = data_train.drop(['label'], axis=1).values, data_train['label'].values.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((42000, 784), (42000, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "class AutoencoderKmeans(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=20, num_clusters=10):\n",
    "        super().__init__()\n",
    "        # Global timestamp\n",
    "        self.t = 0\n",
    "        \n",
    "        # Hyper parameters\n",
    "        self.num_clusters = num_clusters\n",
    "        self.encoding_dim = encoding_dim\n",
    "        \n",
    "        # Define AE architecture\n",
    "        self.fc1 = nn.Linear(input_dim, self.encoding_dim)\n",
    "        self.fc2 = nn.Linear(self.encoding_dim, input_dim)\n",
    "        \n",
    "        # Initialize cluster centers\n",
    "        self.centers = torch.from_numpy(np.random.uniform(0, 1, (self.num_clusters, self.encoding_dim))).type(torch.FloatTensor)\n",
    "        self.clusters = None\n",
    "        self.AKoutput = namedtuple('AKoutput', 'k_means_loss clusters data_hidden output')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        data_hidden = x.clone()\n",
    "        clusters = self.get_clusters(data_hidden)\n",
    "        k_means_loss = self.get_kmeans_loss(data_hidden, clusters)\n",
    "        self.centers = self.update_centers(data_hidden, clusters)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "\n",
    "        output = x\n",
    "        \n",
    "        self.t += 1\n",
    "        \n",
    "        output_tuple = self.AKoutput(k_means_loss=k_means_loss, clusters=clusters, data_hidden=data_hidden, output=output)\n",
    "#         return k_means_loss, clusters, data_hidden, output\n",
    "        return output_tuple\n",
    "    \n",
    "    def get_clusters(self, dataset):\n",
    "        clusters = []\n",
    "        if isinstance(self.centers, torch.autograd.Variable):\n",
    "            self.centers = self.centers.data\n",
    "        for data in dataset[:10000]:\n",
    "            clusters.append(((data - Variable(self.centers, requires_grad=False))**2).sum(1).min(0)[1][0])\n",
    "        return clusters\n",
    "    \n",
    "    def get_kmeans_loss(self, dataset, clusters):\n",
    "        loss = [0 for i in range(self.num_clusters)]\n",
    "        \n",
    "        for i, cluster in enumerate(clusters):\n",
    "            loss[int(cluster.data)] += ((dataset[i] - Variable(self.centers[int(cluster.data)], requires_grad=False))**2).sum()\n",
    "            \n",
    "        return sum(loss)\n",
    "    \n",
    "    def update_centers(self, dataset, clusters):\n",
    "        data_by_cluster = [[] for i in range(self.num_clusters)]\n",
    "        \n",
    "        for i, x in enumerate(clusters):\n",
    "            data_by_cluster[int(x.data)].append(dataset[i])\n",
    "        \n",
    "#         print(len(data_by_cluster))\n",
    "        \n",
    "#         for i in range(len(data_by_cluster)):\n",
    "#             print(len(data_by_cluster[i]))\n",
    "        \n",
    "        centers = [sum(data) / len(data) for data in data_by_cluster]\n",
    "        centers = torch.stack(centers)\n",
    "        \n",
    "        return centers\n",
    "        \n",
    "    @staticmethod\n",
    "    def __square_loss(v1, v2):\n",
    "        loss = ((v1 - v2)**2).sum(1)[0]\n",
    "        \n",
    "        return loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "\n",
    "model = AutoencoderKmeans(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "torch.Size([20, 784])\n",
      "torch.Size([20])\n",
      "torch.Size([784, 20])\n",
      "torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "print(len(list(model.parameters())))\n",
    "print(list(model.parameters())[0].size())\n",
    "print(list(model.parameters())[1].size())\n",
    "print(list(model.parameters())[2].size())\n",
    "print(list(model.parameters())[3].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "lambd = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Reconstruction Loss: 8365.362976209253, K-means Loss: 5452.10546875\n",
      "Epoch 1\n",
      "Reconstruction Loss: 7904.123085062411, K-means Loss: 2796.222900390625\n",
      "Epoch 2\n",
      "Reconstruction Loss: 7714.672661175864, K-means Loss: 1983.618408203125\n",
      "Epoch 3\n",
      "Reconstruction Loss: 7618.877060629487, K-means Loss: 1657.6378173828125\n",
      "Epoch 4\n",
      "Reconstruction Loss: 7551.840899174778, K-means Loss: 1463.075927734375\n",
      "Epoch 5\n",
      "Reconstruction Loss: 7499.758177631087, K-means Loss: 1324.2828369140625\n",
      "Epoch 6\n",
      "Reconstruction Loss: 7456.640709946238, K-means Loss: 1216.552001953125\n",
      "Epoch 7\n",
      "Reconstruction Loss: 7419.561067788388, K-means Loss: 1130.08154296875\n",
      "Epoch 8\n",
      "Reconstruction Loss: 7386.525427188499, K-means Loss: 1058.3455810546875\n",
      "Epoch 9\n",
      "Reconstruction Loss: 7356.04713053859, K-means Loss: 997.6681518554688\n",
      "Epoch 10\n",
      "Reconstruction Loss: 7326.881732993461, K-means Loss: 945.389892578125\n",
      "Epoch 11\n",
      "Reconstruction Loss: 7298.084325823398, K-means Loss: 899.427978515625\n",
      "Epoch 12\n",
      "Reconstruction Loss: 7269.044816533116, K-means Loss: 858.5211791992188\n",
      "Epoch 13\n",
      "Reconstruction Loss: 7239.281758928039, K-means Loss: 821.118408203125\n",
      "Epoch 14\n",
      "Reconstruction Loss: 7208.527674590975, K-means Loss: 786.4647216796875\n",
      "Epoch 15\n",
      "Reconstruction Loss: 7176.469685328056, K-means Loss: 754.0723876953125\n",
      "Epoch 16\n",
      "Reconstruction Loss: 7142.810583734089, K-means Loss: 723.9539184570312\n",
      "Epoch 17\n",
      "Reconstruction Loss: 7107.285374941487, K-means Loss: 696.2972412109375\n",
      "Epoch 18\n",
      "Reconstruction Loss: 7069.6812487878215, K-means Loss: 670.8909912109375\n",
      "Epoch 19\n",
      "Reconstruction Loss: 7029.902782585754, K-means Loss: 647.5091552734375\n",
      "Epoch 20\n",
      "Reconstruction Loss: 6987.540987262936, K-means Loss: 626.4423217773438\n",
      "Epoch 21\n",
      "Reconstruction Loss: 6942.28648543325, K-means Loss: 607.2772216796875\n",
      "Epoch 22\n",
      "Reconstruction Loss: 6894.114938284893, K-means Loss: 589.8102416992188\n",
      "Epoch 23\n",
      "Reconstruction Loss: 6842.746775524516, K-means Loss: 574.2626953125\n",
      "Epoch 24\n",
      "Reconstruction Loss: 6788.165593303709, K-means Loss: 559.3460083007812\n",
      "Epoch 25\n",
      "Reconstruction Loss: 6729.886654680045, K-means Loss: 544.9631958007812\n",
      "Epoch 26\n",
      "Reconstruction Loss: 6667.4927461816915, K-means Loss: 532.127197265625\n",
      "Epoch 27\n",
      "Reconstruction Loss: 6600.957749028438, K-means Loss: 521.227294921875\n",
      "Epoch 28\n",
      "Reconstruction Loss: 6530.667398905639, K-means Loss: 511.4821472167969\n",
      "Epoch 29\n",
      "Reconstruction Loss: 6456.913991315522, K-means Loss: 503.3827209472656\n",
      "Epoch 30\n",
      "Reconstruction Loss: 6380.371492659012, K-means Loss: 496.4529113769531\n",
      "Epoch 31\n",
      "Reconstruction Loss: 6301.86394434461, K-means Loss: 489.94598388671875\n",
      "Epoch 32\n",
      "Reconstruction Loss: 6222.072255346667, K-means Loss: 483.7730712890625\n",
      "Epoch 33\n",
      "Reconstruction Loss: 6141.748346536161, K-means Loss: 477.1966552734375\n",
      "Epoch 34\n",
      "Reconstruction Loss: 6061.402892203479, K-means Loss: 470.0922546386719\n",
      "Epoch 35\n",
      "Reconstruction Loss: 5981.219259786419, K-means Loss: 462.81927490234375\n",
      "Epoch 36\n",
      "Reconstruction Loss: 5901.481740518492, K-means Loss: 455.3661193847656\n",
      "Epoch 37\n",
      "Reconstruction Loss: 5822.4403942073595, K-means Loss: 447.6094970703125\n",
      "Epoch 38\n",
      "Reconstruction Loss: 5744.208414922412, K-means Loss: 439.7987060546875\n",
      "Epoch 39\n",
      "Reconstruction Loss: 5666.8417379684615, K-means Loss: 432.5165100097656\n",
      "Epoch 40\n",
      "Reconstruction Loss: 5590.929013164881, K-means Loss: 425.1910400390625\n",
      "Epoch 41\n",
      "Reconstruction Loss: 5516.769303128231, K-means Loss: 417.8128967285156\n",
      "Epoch 42\n",
      "Reconstruction Loss: 5444.607778767576, K-means Loss: 410.1526184082031\n",
      "Epoch 43\n",
      "Reconstruction Loss: 5374.688166195196, K-means Loss: 402.03302001953125\n",
      "Epoch 44\n",
      "Reconstruction Loss: 5306.914789835445, K-means Loss: 394.24176025390625\n",
      "Epoch 45\n",
      "Reconstruction Loss: 5241.334616194038, K-means Loss: 387.01873779296875\n",
      "Epoch 46\n",
      "Reconstruction Loss: 5178.0328061914115, K-means Loss: 380.62060546875\n",
      "Epoch 47\n",
      "Reconstruction Loss: 5117.141378744414, K-means Loss: 374.8134765625\n",
      "Epoch 48\n",
      "Reconstruction Loss: 5058.926140682674, K-means Loss: 368.9489440917969\n",
      "Epoch 49\n",
      "Reconstruction Loss: 5003.525943713012, K-means Loss: 362.9655456542969\n",
      "Epoch 50\n",
      "Reconstruction Loss: 4951.042853474898, K-means Loss: 356.5035095214844\n",
      "Epoch 51\n",
      "Reconstruction Loss: 4901.63450137475, K-means Loss: 349.2319030761719\n",
      "Epoch 52\n",
      "Reconstruction Loss: 4855.09048779036, K-means Loss: 341.71795654296875\n",
      "Epoch 53\n",
      "Reconstruction Loss: 4811.183081445407, K-means Loss: 334.1011047363281\n",
      "Epoch 54\n",
      "Reconstruction Loss: 4769.577488432695, K-means Loss: 326.9184265136719\n",
      "Epoch 55\n",
      "Reconstruction Loss: 4729.781282680754, K-means Loss: 320.6853332519531\n",
      "Epoch 56\n",
      "Reconstruction Loss: 4691.9974501726265, K-means Loss: 314.5816650390625\n",
      "Epoch 57\n",
      "Reconstruction Loss: 4656.025915562321, K-means Loss: 308.9934997558594\n",
      "Epoch 58\n",
      "Reconstruction Loss: 4621.999386135243, K-means Loss: 303.45379638671875\n",
      "Epoch 59\n",
      "Reconstruction Loss: 4589.78816121756, K-means Loss: 298.2546081542969\n",
      "Epoch 60\n",
      "Reconstruction Loss: 4559.434001225523, K-means Loss: 293.087890625\n",
      "Epoch 61\n",
      "Reconstruction Loss: 4530.92890703884, K-means Loss: 287.7746887207031\n",
      "Epoch 62\n",
      "Reconstruction Loss: 4504.28174180266, K-means Loss: 282.1917419433594\n",
      "Epoch 63\n",
      "Reconstruction Loss: 4479.322685084595, K-means Loss: 276.75299072265625\n",
      "Epoch 64\n",
      "Reconstruction Loss: 4455.846518551215, K-means Loss: 271.40234375\n",
      "Epoch 65\n",
      "Reconstruction Loss: 4433.69595651792, K-means Loss: 266.3241271972656\n",
      "Epoch 66\n",
      "Reconstruction Loss: 4412.703547529238, K-means Loss: 261.5296325683594\n",
      "Epoch 67\n",
      "Reconstruction Loss: 4392.950617343993, K-means Loss: 256.79815673828125\n",
      "Epoch 68\n",
      "Reconstruction Loss: 4374.330126094289, K-means Loss: 252.4213409423828\n",
      "Epoch 69\n",
      "Reconstruction Loss: 4356.811232296837, K-means Loss: 248.27626037597656\n",
      "Epoch 70\n",
      "Reconstruction Loss: 4340.339070480407, K-means Loss: 244.3086395263672\n",
      "Epoch 71\n",
      "Reconstruction Loss: 4324.8185488364, K-means Loss: 240.60305786132812\n",
      "Epoch 72\n",
      "Reconstruction Loss: 4310.259521782922, K-means Loss: 237.01649475097656\n",
      "Epoch 73\n",
      "Reconstruction Loss: 4296.569148821444, K-means Loss: 233.6919403076172\n",
      "Epoch 74\n",
      "Reconstruction Loss: 4283.686946404733, K-means Loss: 230.5950469970703\n",
      "Epoch 75\n",
      "Reconstruction Loss: 4271.6887101968705, K-means Loss: 227.48568725585938\n",
      "Epoch 76\n",
      "Reconstruction Loss: 4260.50522937526, K-means Loss: 224.4915008544922\n",
      "Epoch 77\n",
      "Reconstruction Loss: 4250.084499016952, K-means Loss: 221.519287109375\n",
      "Epoch 78\n",
      "Reconstruction Loss: 4240.389343003816, K-means Loss: 218.58389282226562\n",
      "Epoch 79\n",
      "Reconstruction Loss: 4231.401829600148, K-means Loss: 215.6637725830078\n",
      "Epoch 80\n",
      "Reconstruction Loss: 4223.018622383431, K-means Loss: 212.8545379638672\n",
      "Epoch 81\n",
      "Reconstruction Loss: 4215.214778181292, K-means Loss: 210.0987548828125\n",
      "Epoch 82\n",
      "Reconstruction Loss: 4208.004951446044, K-means Loss: 207.27883911132812\n",
      "Epoch 83\n",
      "Reconstruction Loss: 4201.329277809313, K-means Loss: 204.4810333251953\n",
      "Epoch 84\n",
      "Reconstruction Loss: 4195.09272847609, K-means Loss: 201.787841796875\n",
      "Epoch 85\n",
      "Reconstruction Loss: 4189.267974153541, K-means Loss: 199.20059204101562\n",
      "Epoch 86\n",
      "Reconstruction Loss: 4183.789049600294, K-means Loss: 196.73825073242188\n",
      "Epoch 87\n",
      "Reconstruction Loss: 4178.672474887824, K-means Loss: 194.35003662109375\n",
      "Epoch 88\n",
      "Reconstruction Loss: 4173.838857433551, K-means Loss: 192.11387634277344\n",
      "Epoch 89\n",
      "Reconstruction Loss: 4169.2847518454255, K-means Loss: 189.97113037109375\n",
      "Epoch 90\n",
      "Reconstruction Loss: 4165.017907525079, K-means Loss: 187.88352966308594\n",
      "Epoch 91\n",
      "Reconstruction Loss: 4161.04605031249, K-means Loss: 185.80348205566406\n",
      "Epoch 92\n",
      "Reconstruction Loss: 4157.377961471354, K-means Loss: 183.69189453125\n",
      "Epoch 93\n",
      "Reconstruction Loss: 4153.9610004422175, K-means Loss: 181.63404846191406\n",
      "Epoch 94\n",
      "Reconstruction Loss: 4150.762845040643, K-means Loss: 179.6411895751953\n",
      "Epoch 95\n",
      "Reconstruction Loss: 4147.751302943567, K-means Loss: 177.74278259277344\n",
      "Epoch 96\n",
      "Reconstruction Loss: 4144.890152380247, K-means Loss: 175.96994018554688\n",
      "Epoch 97\n",
      "Reconstruction Loss: 4142.166707154036, K-means Loss: 174.2930145263672\n",
      "Epoch 98\n",
      "Reconstruction Loss: 4139.620713043179, K-means Loss: 172.62013244628906\n",
      "Epoch 99\n",
      "Reconstruction Loss: 4137.255072416174, K-means Loss: 170.92759704589844\n",
      "Epoch 100\n",
      "Reconstruction Loss: 4135.114670967343, K-means Loss: 169.06739807128906\n",
      "Epoch 101\n",
      "Reconstruction Loss: 4133.182735098854, K-means Loss: 167.06675720214844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 102\n",
      "Reconstruction Loss: 4131.440363491141, K-means Loss: 164.91831970214844\n",
      "Epoch 103\n",
      "Reconstruction Loss: 4129.847716716681, K-means Loss: 162.7261199951172\n",
      "Epoch 104\n",
      "Reconstruction Loss: 4128.366109304334, K-means Loss: 160.59738159179688\n",
      "Epoch 105\n",
      "Reconstruction Loss: 4126.939202342303, K-means Loss: 158.61216735839844\n",
      "Epoch 106\n",
      "Reconstruction Loss: 4125.585361741569, K-means Loss: 156.7139434814453\n",
      "Epoch 107\n",
      "Reconstruction Loss: 4124.137675982547, K-means Loss: 155.1031951904297\n",
      "Epoch 108\n",
      "Reconstruction Loss: 4122.609544792106, K-means Loss: 153.6694793701172\n",
      "Epoch 109\n",
      "Reconstruction Loss: 4121.070086120972, K-means Loss: 152.3441619873047\n",
      "Epoch 110\n",
      "Reconstruction Loss: 4119.571693845858, K-means Loss: 151.1218719482422\n",
      "Epoch 111\n",
      "Reconstruction Loss: 4118.181804165205, K-means Loss: 149.90374755859375\n",
      "Epoch 112\n",
      "Reconstruction Loss: 4116.92357943993, K-means Loss: 148.6549530029297\n",
      "Epoch 113\n",
      "Reconstruction Loss: 4115.766551298723, K-means Loss: 147.40878295898438\n",
      "Epoch 114\n",
      "Reconstruction Loss: 4114.68858643911, K-means Loss: 146.1703643798828\n",
      "Epoch 115\n",
      "Reconstruction Loss: 4113.6757006282905, K-means Loss: 144.94313049316406\n",
      "Epoch 116\n",
      "Reconstruction Loss: 4112.70383227089, K-means Loss: 143.76229858398438\n",
      "Epoch 117\n",
      "Reconstruction Loss: 4111.7680789711185, K-means Loss: 142.6037139892578\n",
      "Epoch 118\n",
      "Reconstruction Loss: 4110.89264094538, K-means Loss: 141.423583984375\n",
      "Epoch 119\n",
      "Reconstruction Loss: 4110.058126039861, K-means Loss: 140.24366760253906\n",
      "Epoch 120\n",
      "Reconstruction Loss: 4109.279711423052, K-means Loss: 139.04222106933594\n",
      "Epoch 121\n",
      "Reconstruction Loss: 4108.52792107616, K-means Loss: 137.87234497070312\n",
      "Epoch 122\n",
      "Reconstruction Loss: 4107.775887508556, K-means Loss: 136.75733947753906\n",
      "Epoch 123\n",
      "Reconstruction Loss: 4107.023573907898, K-means Loss: 135.68280029296875\n",
      "Epoch 124\n",
      "Reconstruction Loss: 4106.283408029545, K-means Loss: 134.64804077148438\n",
      "Epoch 125\n",
      "Reconstruction Loss: 4105.540081092457, K-means Loss: 133.66917419433594\n",
      "Epoch 126\n",
      "Reconstruction Loss: 4104.800358161433, K-means Loss: 132.7384490966797\n",
      "Epoch 127\n",
      "Reconstruction Loss: 4104.078683982678, K-means Loss: 131.82943725585938\n",
      "Epoch 128\n",
      "Reconstruction Loss: 4103.395520346466, K-means Loss: 130.9210205078125\n",
      "Epoch 129\n",
      "Reconstruction Loss: 4102.72816582354, K-means Loss: 130.04550170898438\n",
      "Epoch 130\n",
      "Reconstruction Loss: 4102.09912261667, K-means Loss: 129.16798400878906\n",
      "Epoch 131\n",
      "Reconstruction Loss: 4101.483529564082, K-means Loss: 128.31063842773438\n",
      "Epoch 132\n",
      "Reconstruction Loss: 4100.879499161776, K-means Loss: 127.47896575927734\n",
      "Epoch 133\n",
      "Reconstruction Loss: 4100.2815115555795, K-means Loss: 126.66129302978516\n",
      "Epoch 134\n",
      "Reconstruction Loss: 4099.697649207261, K-means Loss: 125.86579895019531\n",
      "Epoch 135\n",
      "Reconstruction Loss: 4099.114447943634, K-means Loss: 125.08939361572266\n",
      "Epoch 136\n",
      "Reconstruction Loss: 4098.548669056891, K-means Loss: 124.32809448242188\n",
      "Epoch 137\n",
      "Reconstruction Loss: 4097.968714586222, K-means Loss: 123.60338592529297\n",
      "Epoch 138\n",
      "Reconstruction Loss: 4097.411150359675, K-means Loss: 122.8845443725586\n",
      "Epoch 139\n",
      "Reconstruction Loss: 4096.834176771389, K-means Loss: 122.20491790771484\n",
      "Epoch 140\n",
      "Reconstruction Loss: 4096.283870902001, K-means Loss: 121.5316162109375\n",
      "Epoch 141\n",
      "Reconstruction Loss: 4095.7213264693105, K-means Loss: 120.88420867919922\n",
      "Epoch 142\n",
      "Reconstruction Loss: 4095.205791867107, K-means Loss: 120.23572540283203\n",
      "Epoch 143\n",
      "Reconstruction Loss: 4094.6520212084074, K-means Loss: 119.64422607421875\n",
      "Epoch 144\n",
      "Reconstruction Loss: 4094.1576773016905, K-means Loss: 119.0575942993164\n",
      "Epoch 145\n",
      "Reconstruction Loss: 4093.599220420521, K-means Loss: 118.56536865234375\n",
      "Epoch 146\n",
      "Reconstruction Loss: 4093.1189029353686, K-means Loss: 118.09031677246094\n",
      "Epoch 147\n",
      "Reconstruction Loss: 4092.553485663004, K-means Loss: 117.73564910888672\n",
      "Epoch 148\n",
      "Reconstruction Loss: 4092.0959676229263, K-means Loss: 117.3787841796875\n",
      "Epoch 149\n",
      "Reconstruction Loss: 4091.525799534046, K-means Loss: 117.11106872558594\n",
      "Epoch 150\n",
      "Reconstruction Loss: 4091.0893875032366, K-means Loss: 116.75304412841797\n",
      "Epoch 151\n",
      "Reconstruction Loss: 4090.493431143746, K-means Loss: 116.37520599365234\n",
      "Epoch 152\n",
      "Reconstruction Loss: 4090.0100473911284, K-means Loss: 115.77073669433594\n",
      "Epoch 153\n",
      "Reconstruction Loss: 4089.4023344729826, K-means Loss: 115.1138687133789\n",
      "Epoch 154\n",
      "Reconstruction Loss: 4088.879373450778, K-means Loss: 114.35043334960938\n",
      "Epoch 155\n",
      "Reconstruction Loss: 4088.3026348470316, K-means Loss: 113.59651947021484\n",
      "Epoch 156\n",
      "Reconstruction Loss: 4087.7738021633513, K-means Loss: 112.8943862915039\n",
      "Epoch 157\n",
      "Reconstruction Loss: 4087.2186668618824, K-means Loss: 112.23675537109375\n",
      "Epoch 158\n",
      "Reconstruction Loss: 4086.6707530560616, K-means Loss: 111.66492462158203\n",
      "Epoch 159\n",
      "Reconstruction Loss: 4086.1094041652304, K-means Loss: 111.12762451171875\n",
      "Epoch 160\n",
      "Reconstruction Loss: 4085.546490376582, K-means Loss: 110.64995574951172\n",
      "Epoch 161\n",
      "Reconstruction Loss: 4084.981874793916, K-means Loss: 110.18309020996094\n",
      "Epoch 162\n",
      "Reconstruction Loss: 4084.4157541107734, K-means Loss: 109.75054931640625\n",
      "Epoch 163\n",
      "Reconstruction Loss: 4083.8539057663456, K-means Loss: 109.31748962402344\n",
      "Epoch 164\n",
      "Reconstruction Loss: 4083.280829855058, K-means Loss: 108.92109680175781\n",
      "Epoch 165\n",
      "Reconstruction Loss: 4082.7043751209226, K-means Loss: 108.53030395507812\n",
      "Epoch 166\n",
      "Reconstruction Loss: 4082.1179866518696, K-means Loss: 108.16378784179688\n",
      "Epoch 167\n",
      "Reconstruction Loss: 4081.5309912590105, K-means Loss: 107.80017852783203\n",
      "Epoch 168\n",
      "Reconstruction Loss: 4080.934551664837, K-means Loss: 107.45834350585938\n",
      "Epoch 169\n",
      "Reconstruction Loss: 4080.3416563744063, K-means Loss: 107.11510467529297\n",
      "Epoch 170\n",
      "Reconstruction Loss: 4079.739464507868, K-means Loss: 106.79188537597656\n",
      "Epoch 171\n",
      "Reconstruction Loss: 4079.137686155811, K-means Loss: 106.4696273803711\n",
      "Epoch 172\n",
      "Reconstruction Loss: 4078.5295051736034, K-means Loss: 106.16185760498047\n",
      "Epoch 173\n",
      "Reconstruction Loss: 4077.9192423063996, K-means Loss: 105.85693359375\n",
      "Epoch 174\n",
      "Reconstruction Loss: 4077.297280019616, K-means Loss: 105.56966400146484\n",
      "Epoch 175\n",
      "Reconstruction Loss: 4076.67202337348, K-means Loss: 105.28736114501953\n",
      "Epoch 176\n",
      "Reconstruction Loss: 4076.036089257293, K-means Loss: 105.02029418945312\n",
      "Epoch 177\n",
      "Reconstruction Loss: 4075.395482095159, K-means Loss: 104.75861358642578\n",
      "Epoch 178\n",
      "Reconstruction Loss: 4074.742302788958, K-means Loss: 104.51265716552734\n",
      "Epoch 179\n",
      "Reconstruction Loss: 4074.0791941283815, K-means Loss: 104.27661895751953\n",
      "Epoch 180\n",
      "Reconstruction Loss: 4073.400513331153, K-means Loss: 104.05865478515625\n",
      "Epoch 181\n",
      "Reconstruction Loss: 4072.7168368123303, K-means Loss: 103.84517669677734\n",
      "Epoch 182\n",
      "Reconstruction Loss: 4072.0228989391608, K-means Loss: 103.64445495605469\n",
      "Epoch 183\n",
      "Reconstruction Loss: 4071.315026621142, K-means Loss: 103.45735931396484\n",
      "Epoch 184\n",
      "Reconstruction Loss: 4070.596754180215, K-means Loss: 103.28376007080078\n",
      "Epoch 185\n",
      "Reconstruction Loss: 4069.8715380116, K-means Loss: 103.11692810058594\n",
      "Epoch 186\n",
      "Reconstruction Loss: 4069.1411339799843, K-means Loss: 102.95677947998047\n",
      "Epoch 187\n",
      "Reconstruction Loss: 4068.407942649009, K-means Loss: 102.79796600341797\n",
      "Epoch 188\n",
      "Reconstruction Loss: 4067.6658362298003, K-means Loss: 102.64879608154297\n",
      "Epoch 189\n",
      "Reconstruction Loss: 4066.9149533203054, K-means Loss: 102.5063705444336\n",
      "Epoch 190\n",
      "Reconstruction Loss: 4066.1534800679283, K-means Loss: 102.37561798095703\n",
      "Epoch 191\n",
      "Reconstruction Loss: 4065.384375056024, K-means Loss: 102.2506103515625\n",
      "Epoch 192\n",
      "Reconstruction Loss: 4064.6016760359257, K-means Loss: 102.13953399658203\n",
      "Epoch 193\n",
      "Reconstruction Loss: 4063.8101121804953, K-means Loss: 102.03466796875\n",
      "Epoch 194\n",
      "Reconstruction Loss: 4063.0064461806273, K-means Loss: 101.94259643554688\n",
      "Epoch 195\n",
      "Reconstruction Loss: 4062.195988313986, K-means Loss: 101.85409545898438\n",
      "Epoch 196\n",
      "Reconstruction Loss: 4061.372569927329, K-means Loss: 101.77816772460938\n",
      "Epoch 197\n",
      "Reconstruction Loss: 4060.5418014635707, K-means Loss: 101.70713806152344\n",
      "Epoch 198\n",
      "Reconstruction Loss: 4059.6950464764136, K-means Loss: 101.65176391601562\n",
      "Epoch 199\n",
      "Reconstruction Loss: 4058.839914994498, K-means Loss: 101.60165405273438\n",
      "Epoch 200\n",
      "Reconstruction Loss: 4057.9688696516782, K-means Loss: 101.5670394897461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 201\n",
      "Reconstruction Loss: 4057.0919883548154, K-means Loss: 101.53502655029297\n",
      "Epoch 202\n",
      "Reconstruction Loss: 4056.1977790729547, K-means Loss: 101.52021026611328\n",
      "Epoch 203\n",
      "Reconstruction Loss: 4055.2949368615164, K-means Loss: 101.50951385498047\n",
      "Epoch 204\n",
      "Reconstruction Loss: 4054.3724900692628, K-means Loss: 101.5184097290039\n",
      "Epoch 205\n",
      "Reconstruction Loss: 4053.442099709534, K-means Loss: 101.53080749511719\n",
      "Epoch 206\n",
      "Reconstruction Loss: 4052.4911127529053, K-means Loss: 101.56431579589844\n",
      "Epoch 207\n",
      "Reconstruction Loss: 4051.5364965611443, K-means Loss: 101.59666442871094\n",
      "Epoch 208\n",
      "Reconstruction Loss: 4050.5605223088914, K-means Loss: 101.65213775634766\n",
      "Epoch 209\n",
      "Reconstruction Loss: 4049.5815840476607, K-means Loss: 101.70524597167969\n",
      "Epoch 210\n",
      "Reconstruction Loss: 4048.579292014539, K-means Loss: 101.78559112548828\n",
      "Epoch 211\n",
      "Reconstruction Loss: 4047.5785084197546, K-means Loss: 101.86019897460938\n",
      "Epoch 212\n",
      "Reconstruction Loss: 4046.547758883615, K-means Loss: 101.97209930419922\n",
      "Epoch 213\n",
      "Reconstruction Loss: 4045.5220910323687, K-means Loss: 102.07752990722656\n",
      "Epoch 214\n",
      "Reconstruction Loss: 4044.46319355013, K-means Loss: 102.22969818115234\n",
      "Epoch 215\n",
      "Reconstruction Loss: 4043.414786620206, K-means Loss: 102.37459564208984\n",
      "Epoch 216\n",
      "Reconstruction Loss: 4042.3285433841947, K-means Loss: 102.5823974609375\n",
      "Epoch 217\n",
      "Reconstruction Loss: 4041.262485124963, K-means Loss: 102.78071594238281\n",
      "Epoch 218\n",
      "Reconstruction Loss: 4040.1491452250157, K-means Loss: 103.06877899169922\n",
      "Epoch 219\n",
      "Reconstruction Loss: 4039.065863494835, K-means Loss: 103.34797668457031\n",
      "Epoch 220\n",
      "Reconstruction Loss: 4037.919677225122, K-means Loss: 103.74905395507812\n",
      "Epoch 221\n",
      "Reconstruction Loss: 4036.8217225452318, K-means Loss: 104.12593078613281\n",
      "Epoch 222\n",
      "Reconstruction Loss: 4035.64105319787, K-means Loss: 104.64030456542969\n",
      "Epoch 223\n",
      "Reconstruction Loss: 4034.5292743275854, K-means Loss: 105.07044982910156\n",
      "Epoch 224\n",
      "Reconstruction Loss: 4033.303224610176, K-means Loss: 105.6098403930664\n",
      "Epoch 225\n",
      "Reconstruction Loss: 4032.1711063518846, K-means Loss: 105.95092010498047\n",
      "Epoch 226\n",
      "Reconstruction Loss: 4030.8935619388267, K-means Loss: 106.34077453613281\n",
      "Epoch 227\n",
      "Reconstruction Loss: 4029.7316854558608, K-means Loss: 106.4436264038086\n",
      "Epoch 228\n",
      "Reconstruction Loss: 4028.4088969594222, K-means Loss: 106.59308624267578\n",
      "Epoch 229\n",
      "Reconstruction Loss: 4027.211158349698, K-means Loss: 106.4898452758789\n",
      "Epoch 230\n",
      "Reconstruction Loss: 4025.860067496326, K-means Loss: 106.50419616699219\n",
      "Epoch 231\n",
      "Reconstruction Loss: 4024.6281626450227, K-means Loss: 106.36872863769531\n",
      "Epoch 232\n",
      "Reconstruction Loss: 4023.2618020098193, K-means Loss: 106.40448760986328\n",
      "Epoch 233\n",
      "Reconstruction Loss: 4021.995392752015, K-means Loss: 106.36226654052734\n",
      "Epoch 234\n",
      "Reconstruction Loss: 4020.6193617707077, K-means Loss: 106.4824447631836\n",
      "Epoch 235\n",
      "Reconstruction Loss: 4019.3155115942554, K-means Loss: 106.55622100830078\n",
      "Epoch 236\n",
      "Reconstruction Loss: 4017.9226941379225, K-means Loss: 106.76004791259766\n",
      "Epoch 237\n",
      "Reconstruction Loss: 4016.5798023813195, K-means Loss: 106.92827606201172\n",
      "Epoch 238\n",
      "Reconstruction Loss: 4015.16297490526, K-means Loss: 107.1944351196289\n",
      "Epoch 239\n",
      "Reconstruction Loss: 4013.7834692130355, K-means Loss: 107.42813873291016\n",
      "Epoch 240\n",
      "Reconstruction Loss: 4012.342231856865, K-means Loss: 107.73484802246094\n",
      "Epoch 241\n",
      "Reconstruction Loss: 4010.9244747169464, K-means Loss: 108.01681518554688\n",
      "Epoch 242\n",
      "Reconstruction Loss: 4009.4559918584255, K-means Loss: 108.35395812988281\n",
      "Epoch 243\n",
      "Reconstruction Loss: 4008.0014475674816, K-means Loss: 108.67205810546875\n",
      "Epoch 244\n",
      "Reconstruction Loss: 4006.5071360224874, K-means Loss: 109.03018951416016\n",
      "Epoch 245\n",
      "Reconstruction Loss: 4005.0260952721205, K-means Loss: 109.36827850341797\n",
      "Epoch 246\n",
      "Reconstruction Loss: 4003.5088448496567, K-means Loss: 109.73925018310547\n",
      "Epoch 247\n",
      "Reconstruction Loss: 4001.9972373162327, K-means Loss: 110.09716796875\n",
      "Epoch 248\n",
      "Reconstruction Loss: 4000.4488473328943, K-means Loss: 110.48743438720703\n",
      "Epoch 249\n",
      "Reconstruction Loss: 3998.9056960876824, K-means Loss: 110.86518096923828\n",
      "Epoch 250\n",
      "Reconstruction Loss: 3997.331397114449, K-means Loss: 111.26896667480469\n",
      "Epoch 251\n",
      "Reconstruction Loss: 3995.755850032371, K-means Loss: 111.66634368896484\n",
      "Epoch 252\n",
      "Reconstruction Loss: 3994.1547359862698, K-means Loss: 112.08386993408203\n",
      "Epoch 253\n",
      "Reconstruction Loss: 3992.549837868524, K-means Loss: 112.49696350097656\n",
      "Epoch 254\n",
      "Reconstruction Loss: 3990.9226096357834, K-means Loss: 112.92691802978516\n",
      "Epoch 255\n",
      "Reconstruction Loss: 3989.2896944503964, K-means Loss: 113.35344696044922\n",
      "Epoch 256\n",
      "Reconstruction Loss: 3987.635113932627, K-means Loss: 113.79558563232422\n",
      "Epoch 257\n",
      "Reconstruction Loss: 3985.969506574832, K-means Loss: 114.23919677734375\n",
      "Epoch 258\n",
      "Reconstruction Loss: 3984.2797031208443, K-means Loss: 114.7009506225586\n",
      "Epoch 259\n",
      "Reconstruction Loss: 3982.576054296357, K-means Loss: 115.16827392578125\n",
      "Epoch 260\n",
      "Reconstruction Loss: 3980.8545946942645, K-means Loss: 115.64751434326172\n",
      "Epoch 261\n",
      "Reconstruction Loss: 3979.1261790193507, K-means Loss: 116.12506866455078\n",
      "Epoch 262\n",
      "Reconstruction Loss: 3977.376163410011, K-means Loss: 116.61734008789062\n",
      "Epoch 263\n",
      "Reconstruction Loss: 3975.620998889428, K-means Loss: 117.10782623291016\n",
      "Epoch 264\n",
      "Reconstruction Loss: 3973.8491017142674, K-means Loss: 117.60824584960938\n",
      "Epoch 265\n",
      "Reconstruction Loss: 3972.0695538019286, K-means Loss: 118.10844421386719\n",
      "Epoch 266\n",
      "Reconstruction Loss: 3970.272222677221, K-means Loss: 118.61993408203125\n",
      "Epoch 267\n",
      "Reconstruction Loss: 3968.468300340756, K-means Loss: 119.1307373046875\n",
      "Epoch 268\n",
      "Reconstruction Loss: 3966.6468165139645, K-means Loss: 119.65258026123047\n",
      "Epoch 269\n",
      "Reconstruction Loss: 3964.818575310369, K-means Loss: 120.173828125\n",
      "Epoch 270\n",
      "Reconstruction Loss: 3962.974484322367, K-means Loss: 120.70457458496094\n",
      "Epoch 271\n",
      "Reconstruction Loss: 3961.123037451072, K-means Loss: 121.23554992675781\n",
      "Epoch 272\n",
      "Reconstruction Loss: 3959.256937006825, K-means Loss: 121.77540588378906\n",
      "Epoch 273\n",
      "Reconstruction Loss: 3957.3794430760668, K-means Loss: 122.31890869140625\n",
      "Epoch 274\n",
      "Reconstruction Loss: 3955.48555103289, K-means Loss: 122.87261962890625\n",
      "Epoch 275\n",
      "Reconstruction Loss: 3953.5787649997815, K-means Loss: 123.4316177368164\n",
      "Epoch 276\n",
      "Reconstruction Loss: 3951.652983307623, K-means Loss: 124.00393676757812\n",
      "Epoch 277\n",
      "Reconstruction Loss: 3949.7133653101027, K-means Loss: 124.58142852783203\n",
      "Epoch 278\n",
      "Reconstruction Loss: 3947.7618232830864, K-means Loss: 125.16655731201172\n",
      "Epoch 279\n",
      "Reconstruction Loss: 3945.8054842307192, K-means Loss: 125.74952697753906\n",
      "Epoch 280\n",
      "Reconstruction Loss: 3943.8317737212947, K-means Loss: 126.34320831298828\n",
      "Epoch 281\n",
      "Reconstruction Loss: 3941.850905053746, K-means Loss: 126.93745422363281\n",
      "Epoch 282\n",
      "Reconstruction Loss: 3939.856524784201, K-means Loss: 127.54071807861328\n",
      "Epoch 283\n",
      "Reconstruction Loss: 3937.8497004755345, K-means Loss: 128.14895629882812\n",
      "Epoch 284\n",
      "Reconstruction Loss: 3935.8300030029363, K-means Loss: 128.76486206054688\n",
      "Epoch 285\n",
      "Reconstruction Loss: 3933.793157444334, K-means Loss: 129.3907928466797\n",
      "Epoch 286\n",
      "Reconstruction Loss: 3931.737321402866, K-means Loss: 130.0301513671875\n",
      "Epoch 287\n",
      "Reconstruction Loss: 3929.657971030675, K-means Loss: 130.68698120117188\n",
      "Epoch 288\n",
      "Reconstruction Loss: 3927.5617916061055, K-means Loss: 131.35997009277344\n",
      "Epoch 289\n",
      "Reconstruction Loss: 3925.4536733439686, K-means Loss: 132.04136657714844\n",
      "Epoch 290\n",
      "Reconstruction Loss: 3923.343173427812, K-means Loss: 132.7230224609375\n",
      "Epoch 291\n",
      "Reconstruction Loss: 3921.2165559915265, K-means Loss: 133.41653442382812\n",
      "Epoch 292\n",
      "Reconstruction Loss: 3919.0919578426588, K-means Loss: 134.11083984375\n",
      "Epoch 293\n",
      "Reconstruction Loss: 3916.9477474563146, K-means Loss: 134.82418823242188\n",
      "Epoch 294\n",
      "Reconstruction Loss: 3914.8014646493107, K-means Loss: 135.5463104248047\n",
      "Epoch 295\n",
      "Reconstruction Loss: 3912.623644428426, K-means Loss: 136.30734252929688\n",
      "Epoch 296\n",
      "Reconstruction Loss: 3910.4447651220185, K-means Loss: 137.08050537109375\n",
      "Epoch 297\n",
      "Reconstruction Loss: 3908.202145880072, K-means Loss: 137.92324829101562\n",
      "Epoch 298\n",
      "Reconstruction Loss: 3905.9364491182187, K-means Loss: 138.80104064941406\n",
      "Epoch 299\n",
      "Reconstruction Loss: 3903.556587337893, K-means Loss: 139.79502868652344\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 300\n",
      "Reconstruction Loss: 3901.089508176899, K-means Loss: 140.88389587402344\n",
      "Epoch 301\n",
      "Reconstruction Loss: 3898.416142942807, K-means Loss: 142.16551208496094\n",
      "Epoch 302\n",
      "Reconstruction Loss: 3895.587309499308, K-means Loss: 143.53611755371094\n",
      "Epoch 303\n",
      "Reconstruction Loss: 3892.425327001123, K-means Loss: 145.09556579589844\n",
      "Epoch 304\n",
      "Reconstruction Loss: 3888.934872213175, K-means Loss: 146.73123168945312\n",
      "Epoch 305\n",
      "Reconstruction Loss: 3884.8515040683365, K-means Loss: 148.55445861816406\n",
      "Epoch 306\n",
      "Reconstruction Loss: 3880.1023441677844, K-means Loss: 150.6791534423828\n",
      "Epoch 307\n",
      "Reconstruction Loss: 3874.4788291961145, K-means Loss: 153.4066619873047\n",
      "Epoch 308\n",
      "Reconstruction Loss: 3868.1601143183807, K-means Loss: 156.7845916748047\n",
      "Epoch 309\n",
      "Reconstruction Loss: 3861.3552484639686, K-means Loss: 160.643798828125\n",
      "Epoch 310\n",
      "Reconstruction Loss: 3854.4731234045935, K-means Loss: 164.4237823486328\n",
      "Epoch 311\n",
      "Reconstruction Loss: 3847.754673199168, K-means Loss: 167.46505737304688\n",
      "Epoch 312\n",
      "Reconstruction Loss: 3841.1532394513915, K-means Loss: 169.91677856445312\n",
      "Epoch 313\n",
      "Reconstruction Loss: 3834.672842716824, K-means Loss: 172.23875427246094\n",
      "Epoch 314\n",
      "Reconstruction Loss: 3828.3449939993325, K-means Loss: 174.44776916503906\n",
      "Epoch 315\n",
      "Reconstruction Loss: 3822.404604890983, K-means Loss: 176.62005615234375\n",
      "Epoch 316\n",
      "Reconstruction Loss: 3816.66971941001, K-means Loss: 178.8811798095703\n",
      "Epoch 317\n",
      "Reconstruction Loss: 3811.428784815939, K-means Loss: 180.82659912109375\n",
      "Epoch 318\n",
      "Reconstruction Loss: 3806.21226477071, K-means Loss: 182.799560546875\n",
      "Epoch 319\n",
      "Reconstruction Loss: 3801.505927431134, K-means Loss: 184.0544891357422\n",
      "Epoch 320\n",
      "Reconstruction Loss: 3796.672227421516, K-means Loss: 185.2832489013672\n",
      "Epoch 321\n",
      "Reconstruction Loss: 3792.1607280461726, K-means Loss: 186.17457580566406\n",
      "Epoch 322\n",
      "Reconstruction Loss: 3787.3808441001756, K-means Loss: 187.47801208496094\n",
      "Epoch 323\n",
      "Reconstruction Loss: 3783.061536047791, K-means Loss: 188.58831787109375\n",
      "Epoch 324\n",
      "Reconstruction Loss: 3778.8573171847397, K-means Loss: 189.718994140625\n",
      "Epoch 325\n",
      "Reconstruction Loss: 3775.3027745820946, K-means Loss: 190.26361083984375\n",
      "Epoch 326\n",
      "Reconstruction Loss: 3771.8461360535957, K-means Loss: 190.7344512939453\n",
      "Epoch 327\n",
      "Reconstruction Loss: 3768.90592166495, K-means Loss: 190.7764129638672\n",
      "Epoch 328\n",
      "Reconstruction Loss: 3765.879624949279, K-means Loss: 190.96482849121094\n",
      "Epoch 329\n",
      "Reconstruction Loss: 3763.1796629937057, K-means Loss: 190.8970489501953\n",
      "Epoch 330\n",
      "Reconstruction Loss: 3760.1751415925196, K-means Loss: 191.1898956298828\n",
      "Epoch 331\n",
      "Reconstruction Loss: 3757.3874483764484, K-means Loss: 191.31915283203125\n",
      "Epoch 332\n",
      "Reconstruction Loss: 3754.240031439017, K-means Loss: 191.860595703125\n",
      "Epoch 333\n",
      "Reconstruction Loss: 3751.301652897927, K-means Loss: 192.2222137451172\n",
      "Epoch 334\n",
      "Reconstruction Loss: 3748.038066302604, K-means Loss: 192.90728759765625\n",
      "Epoch 335\n",
      "Reconstruction Loss: 3745.0980296429334, K-means Loss: 193.28407287597656\n",
      "Epoch 336\n",
      "Reconstruction Loss: 3741.778725328453, K-means Loss: 194.0967559814453\n",
      "Epoch 337\n",
      "Reconstruction Loss: 3738.8499516318548, K-means Loss: 194.57794189453125\n",
      "Epoch 338\n",
      "Reconstruction Loss: 3735.5421589869215, K-means Loss: 195.46359252929688\n",
      "Epoch 339\n",
      "Reconstruction Loss: 3732.7724735956836, K-means Loss: 195.86581420898438\n",
      "Epoch 340\n",
      "Reconstruction Loss: 3729.6287544153447, K-means Loss: 196.66622924804688\n",
      "Epoch 341\n",
      "Reconstruction Loss: 3727.0727974438873, K-means Loss: 196.88082885742188\n",
      "Epoch 342\n",
      "Reconstruction Loss: 3724.0445620142373, K-means Loss: 197.58645629882812\n",
      "Epoch 343\n",
      "Reconstruction Loss: 3721.6121091372443, K-means Loss: 197.71197509765625\n",
      "Epoch 344\n",
      "Reconstruction Loss: 3718.708701531117, K-means Loss: 198.3228302001953\n",
      "Epoch 345\n",
      "Reconstruction Loss: 3716.352527080092, K-means Loss: 198.3828125\n",
      "Epoch 346\n",
      "Reconstruction Loss: 3713.3874306377515, K-means Loss: 199.06082153320312\n",
      "Epoch 347\n",
      "Reconstruction Loss: 3710.9826089114054, K-means Loss: 199.16744995117188\n",
      "Epoch 348\n",
      "Reconstruction Loss: 3708.057965506946, K-means Loss: 199.8179473876953\n",
      "Epoch 349\n",
      "Reconstruction Loss: 3705.6888996855573, K-means Loss: 199.920654296875\n",
      "Epoch 350\n",
      "Reconstruction Loss: 3702.8452758015737, K-means Loss: 200.51927185058594\n",
      "Epoch 351\n",
      "Reconstruction Loss: 3700.5661194931427, K-means Loss: 200.5714569091797\n",
      "Epoch 352\n",
      "Reconstruction Loss: 3697.7543547406513, K-means Loss: 201.18569946289062\n",
      "Epoch 353\n",
      "Reconstruction Loss: 3695.46524898174, K-means Loss: 201.29019165039062\n",
      "Epoch 354\n",
      "Reconstruction Loss: 3692.7078891699452, K-means Loss: 201.8806610107422\n",
      "Epoch 355\n",
      "Reconstruction Loss: 3690.4466455843717, K-means Loss: 201.9950408935547\n",
      "Epoch 356\n",
      "Reconstruction Loss: 3687.7853909128253, K-means Loss: 202.52023315429688\n",
      "Epoch 357\n",
      "Reconstruction Loss: 3685.60977521568, K-means Loss: 202.5919952392578\n",
      "Epoch 358\n",
      "Reconstruction Loss: 3682.986170976715, K-means Loss: 203.13722229003906\n",
      "Epoch 359\n",
      "Reconstruction Loss: 3680.835044008379, K-means Loss: 203.2195281982422\n",
      "Epoch 360\n",
      "Reconstruction Loss: 3678.2921860351944, K-means Loss: 203.7237091064453\n",
      "Epoch 361\n",
      "Reconstruction Loss: 3676.169059972451, K-means Loss: 203.82833862304688\n",
      "Epoch 362\n",
      "Reconstruction Loss: 3673.6451198425952, K-means Loss: 204.3636932373047\n",
      "Epoch 363\n",
      "Reconstruction Loss: 3671.5451043698167, K-means Loss: 204.48880004882812\n",
      "Epoch 364\n",
      "Reconstruction Loss: 3669.016328988737, K-means Loss: 205.071533203125\n",
      "Epoch 365\n",
      "Reconstruction Loss: 3666.889187307206, K-means Loss: 205.2581024169922\n",
      "Epoch 366\n",
      "Reconstruction Loss: 3664.369242842746, K-means Loss: 205.88217163085938\n",
      "Epoch 367\n",
      "Reconstruction Loss: 3662.313718908261, K-means Loss: 206.05596923828125\n",
      "Epoch 368\n",
      "Reconstruction Loss: 3659.8379379493504, K-means Loss: 206.6821746826172\n",
      "Epoch 369\n",
      "Reconstruction Loss: 3657.8642427834543, K-means Loss: 206.81044006347656\n",
      "Epoch 370\n",
      "Reconstruction Loss: 3655.3866130100187, K-means Loss: 207.49049377441406\n",
      "Epoch 371\n",
      "Reconstruction Loss: 3653.4722966240233, K-means Loss: 207.60372924804688\n",
      "Epoch 372\n",
      "Reconstruction Loss: 3651.0439927778853, K-means Loss: 208.2761688232422\n",
      "Epoch 373\n",
      "Reconstruction Loss: 3649.224646811448, K-means Loss: 208.33621215820312\n",
      "Epoch 374\n",
      "Reconstruction Loss: 3646.813934175725, K-means Loss: 209.02462768554688\n",
      "Epoch 375\n",
      "Reconstruction Loss: 3645.014554596003, K-means Loss: 209.08628845214844\n",
      "Epoch 376\n",
      "Reconstruction Loss: 3642.5931932363233, K-means Loss: 209.807373046875\n",
      "Epoch 377\n",
      "Reconstruction Loss: 3640.80356819943, K-means Loss: 209.8669891357422\n",
      "Epoch 378\n",
      "Reconstruction Loss: 3638.4189485268653, K-means Loss: 210.5546875\n",
      "Epoch 379\n",
      "Reconstruction Loss: 3636.634915064413, K-means Loss: 210.60459899902344\n",
      "Epoch 380\n",
      "Reconstruction Loss: 3634.1987831166325, K-means Loss: 211.34677124023438\n",
      "Epoch 381\n",
      "Reconstruction Loss: 3632.4332804268365, K-means Loss: 211.36949157714844\n",
      "Epoch 382\n",
      "Reconstruction Loss: 3629.9993029123434, K-means Loss: 212.10440063476562\n",
      "Epoch 383\n",
      "Reconstruction Loss: 3628.214669423041, K-means Loss: 212.14093017578125\n",
      "Epoch 384\n",
      "Reconstruction Loss: 3625.7913144538684, K-means Loss: 212.87059020996094\n",
      "Epoch 385\n",
      "Reconstruction Loss: 3624.0060194506996, K-means Loss: 212.91712951660156\n",
      "Epoch 386\n",
      "Reconstruction Loss: 3621.594052137582, K-means Loss: 213.6390380859375\n",
      "Epoch 387\n",
      "Reconstruction Loss: 3619.7960291197965, K-means Loss: 213.72125244140625\n",
      "Epoch 388\n",
      "Reconstruction Loss: 3617.41774037509, K-means Loss: 214.4300537109375\n",
      "Epoch 389\n",
      "Reconstruction Loss: 3615.6356150300257, K-means Loss: 214.52255249023438\n",
      "Epoch 390\n",
      "Reconstruction Loss: 3613.26073896065, K-means Loss: 215.24221801757812\n",
      "Epoch 391\n",
      "Reconstruction Loss: 3611.5005432094813, K-means Loss: 215.34213256835938\n",
      "Epoch 392\n",
      "Reconstruction Loss: 3609.147209529359, K-means Loss: 216.0654754638672\n",
      "Epoch 393\n",
      "Reconstruction Loss: 3607.433121368005, K-means Loss: 216.14239501953125\n",
      "Epoch 394\n",
      "Reconstruction Loss: 3605.107180393546, K-means Loss: 216.85479736328125\n",
      "Epoch 395\n",
      "Reconstruction Loss: 3603.483629590964, K-means Loss: 216.8726806640625\n",
      "Epoch 396\n",
      "Reconstruction Loss: 3601.154528557459, K-means Loss: 217.62351989746094\n",
      "Epoch 397\n",
      "Reconstruction Loss: 3599.5851275120276, K-means Loss: 217.63673400878906\n",
      "Epoch 398\n",
      "Reconstruction Loss: 3597.2364915758076, K-means Loss: 218.44569396972656\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 399\n",
      "Reconstruction Loss: 3595.6480581808582, K-means Loss: 218.51129150390625\n",
      "Epoch 400\n",
      "Reconstruction Loss: 3593.285185481646, K-means Loss: 219.3797607421875\n",
      "Epoch 401\n",
      "Reconstruction Loss: 3591.721434718413, K-means Loss: 219.4983367919922\n",
      "Epoch 402\n",
      "Reconstruction Loss: 3589.346924072783, K-means Loss: 220.4402313232422\n",
      "Epoch 403\n",
      "Reconstruction Loss: 3587.8447238084746, K-means Loss: 220.5693817138672\n",
      "Epoch 404\n",
      "Reconstruction Loss: 3585.4353464066894, K-means Loss: 221.6192169189453\n",
      "Epoch 405\n",
      "Reconstruction Loss: 3583.9699085232573, K-means Loss: 221.78111267089844\n",
      "Epoch 406\n",
      "Reconstruction Loss: 3581.5710808880417, K-means Loss: 222.88790893554688\n",
      "Epoch 407\n",
      "Reconstruction Loss: 3580.1458001857154, K-means Loss: 223.05117797851562\n",
      "Epoch 408\n",
      "Reconstruction Loss: 3577.7845815256474, K-means Loss: 224.13539123535156\n",
      "Epoch 409\n",
      "Reconstruction Loss: 3576.3820579839053, K-means Loss: 224.2340087890625\n",
      "Epoch 410\n",
      "Reconstruction Loss: 3574.007700733709, K-means Loss: 225.2611541748047\n",
      "Epoch 411\n",
      "Reconstruction Loss: 3572.647506477105, K-means Loss: 225.1768798828125\n",
      "Epoch 412\n",
      "Reconstruction Loss: 3570.239788112309, K-means Loss: 226.02822875976562\n",
      "Epoch 413\n",
      "Reconstruction Loss: 3568.8919806144313, K-means Loss: 225.7001190185547\n",
      "Epoch 414\n",
      "Reconstruction Loss: 3566.4286299146997, K-means Loss: 226.40000915527344\n",
      "Epoch 415\n",
      "Reconstruction Loss: 3565.054622648232, K-means Loss: 225.93035888671875\n",
      "Epoch 416\n",
      "Reconstruction Loss: 3562.6215747549263, K-means Loss: 226.51629638671875\n",
      "Epoch 417\n",
      "Reconstruction Loss: 3561.25687861381, K-means Loss: 226.0244140625\n",
      "Epoch 418\n",
      "Reconstruction Loss: 3558.8443804365793, K-means Loss: 226.63282775878906\n",
      "Epoch 419\n",
      "Reconstruction Loss: 3557.498171029689, K-means Loss: 226.19827270507812\n",
      "Epoch 420\n",
      "Reconstruction Loss: 3555.094815821095, K-means Loss: 226.8830108642578\n",
      "Epoch 421\n",
      "Reconstruction Loss: 3553.7499864044853, K-means Loss: 226.51455688476562\n",
      "Epoch 422\n",
      "Reconstruction Loss: 3551.397309607736, K-means Loss: 227.2032470703125\n",
      "Epoch 423\n",
      "Reconstruction Loss: 3550.052653653541, K-means Loss: 226.8760986328125\n",
      "Epoch 424\n",
      "Reconstruction Loss: 3547.712260216725, K-means Loss: 227.58770751953125\n",
      "Epoch 425\n",
      "Reconstruction Loss: 3546.3242335900577, K-means Loss: 227.31430053710938\n",
      "Epoch 426\n",
      "Reconstruction Loss: 3543.9890929715284, K-means Loss: 228.0269775390625\n",
      "Epoch 427\n",
      "Reconstruction Loss: 3542.543171224258, K-means Loss: 227.8101348876953\n",
      "Epoch 428\n",
      "Reconstruction Loss: 3540.2264951206303, K-means Loss: 228.51113891601562\n",
      "Epoch 429\n",
      "Reconstruction Loss: 3538.719696440669, K-means Loss: 228.35585021972656\n",
      "Epoch 430\n",
      "Reconstruction Loss: 3536.3858850311553, K-means Loss: 229.09458923339844\n",
      "Epoch 431\n",
      "Reconstruction Loss: 3534.890501506091, K-means Loss: 228.94683837890625\n",
      "Epoch 432\n",
      "Reconstruction Loss: 3532.5854870748444, K-means Loss: 229.66819763183594\n",
      "Epoch 433\n",
      "Reconstruction Loss: 3531.0925330352547, K-means Loss: 229.5253143310547\n",
      "Epoch 434\n",
      "Reconstruction Loss: 3528.789536382946, K-means Loss: 230.26699829101562\n",
      "Epoch 435\n",
      "Reconstruction Loss: 3527.3108586215985, K-means Loss: 230.1334991455078\n",
      "Epoch 436\n",
      "Reconstruction Loss: 3525.062747231286, K-means Loss: 230.83477783203125\n",
      "Epoch 437\n",
      "Reconstruction Loss: 3523.660017134405, K-means Loss: 230.6490936279297\n",
      "Epoch 438\n",
      "Reconstruction Loss: 3521.443013155, K-means Loss: 231.32675170898438\n",
      "Epoch 439\n",
      "Reconstruction Loss: 3520.0205220178223, K-means Loss: 231.17080688476562\n",
      "Epoch 440\n",
      "Reconstruction Loss: 3517.8564228505907, K-means Loss: 231.8207550048828\n",
      "Epoch 441\n",
      "Reconstruction Loss: 3516.433985256623, K-means Loss: 231.68946838378906\n",
      "Epoch 442\n",
      "Reconstruction Loss: 3514.253907585484, K-means Loss: 232.37875366210938\n",
      "Epoch 443\n",
      "Reconstruction Loss: 3512.7909588976077, K-means Loss: 232.3101806640625\n",
      "Epoch 444\n",
      "Reconstruction Loss: 3510.596830782335, K-means Loss: 233.03697204589844\n",
      "Epoch 445\n",
      "Reconstruction Loss: 3509.099571615282, K-means Loss: 233.01275634765625\n",
      "Epoch 446\n",
      "Reconstruction Loss: 3506.885446937781, K-means Loss: 233.78211975097656\n",
      "Epoch 447\n",
      "Reconstruction Loss: 3505.4106342924492, K-means Loss: 233.76271057128906\n",
      "Epoch 448\n",
      "Reconstruction Loss: 3503.188093589572, K-means Loss: 234.5575408935547\n",
      "Epoch 449\n",
      "Reconstruction Loss: 3501.7073037023283, K-means Loss: 234.55072021484375\n",
      "Epoch 450\n",
      "Reconstruction Loss: 3499.4895080781257, K-means Loss: 235.364013671875\n",
      "Epoch 451\n",
      "Reconstruction Loss: 3498.046838761754, K-means Loss: 235.3451385498047\n",
      "Epoch 452\n",
      "Reconstruction Loss: 3495.8581416704997, K-means Loss: 236.15428161621094\n",
      "Epoch 453\n",
      "Reconstruction Loss: 3494.4517342961494, K-means Loss: 236.12950134277344\n",
      "Epoch 454\n",
      "Reconstruction Loss: 3492.3128039097132, K-means Loss: 236.9178466796875\n",
      "Epoch 455\n",
      "Reconstruction Loss: 3490.9453500479017, K-means Loss: 236.8841552734375\n",
      "Epoch 456\n",
      "Reconstruction Loss: 3488.8029769390387, K-means Loss: 237.71217346191406\n",
      "Epoch 457\n",
      "Reconstruction Loss: 3487.4176594030687, K-means Loss: 237.7236785888672\n",
      "Epoch 458\n",
      "Reconstruction Loss: 3485.248131462578, K-means Loss: 238.60073852539062\n",
      "Epoch 459\n",
      "Reconstruction Loss: 3483.8143293004555, K-means Loss: 238.6819305419922\n",
      "Epoch 460\n",
      "Reconstruction Loss: 3481.5949903530536, K-means Loss: 239.64517211914062\n",
      "Epoch 461\n",
      "Reconstruction Loss: 3480.1602253669057, K-means Loss: 239.75086975097656\n",
      "Epoch 462\n",
      "Reconstruction Loss: 3477.9176219406854, K-means Loss: 240.77293395996094\n",
      "Epoch 463\n",
      "Reconstruction Loss: 3476.546970485718, K-means Loss: 240.84796142578125\n",
      "Epoch 464\n",
      "Reconstruction Loss: 3474.351750389789, K-means Loss: 241.85023498535156\n",
      "Epoch 465\n",
      "Reconstruction Loss: 3473.0377367204565, K-means Loss: 241.8887176513672\n",
      "Epoch 466\n",
      "Reconstruction Loss: 3470.870755082483, K-means Loss: 242.86399841308594\n",
      "Epoch 467\n",
      "Reconstruction Loss: 3469.6036328893565, K-means Loss: 242.8577880859375\n",
      "Epoch 468\n",
      "Reconstruction Loss: 3467.3943753454137, K-means Loss: 243.86962890625\n",
      "Epoch 469\n",
      "Reconstruction Loss: 3466.0791436422764, K-means Loss: 243.8966064453125\n",
      "Epoch 470\n",
      "Reconstruction Loss: 3463.850161436685, K-means Loss: 244.91993713378906\n",
      "Epoch 471\n",
      "Reconstruction Loss: 3462.551736638092, K-means Loss: 244.90194702148438\n",
      "Epoch 472\n",
      "Reconstruction Loss: 3460.3004053848654, K-means Loss: 245.8990478515625\n",
      "Epoch 473\n",
      "Reconstruction Loss: 3458.990154319116, K-means Loss: 245.82765197753906\n",
      "Epoch 474\n",
      "Reconstruction Loss: 3456.7346453994614, K-means Loss: 246.7671356201172\n",
      "Epoch 475\n",
      "Reconstruction Loss: 3455.448148072658, K-means Loss: 246.6168975830078\n",
      "Epoch 476\n",
      "Reconstruction Loss: 3453.15933881606, K-means Loss: 247.56053161621094\n",
      "Epoch 477\n",
      "Reconstruction Loss: 3451.8454671726718, K-means Loss: 247.3970947265625\n",
      "Epoch 478\n",
      "Reconstruction Loss: 3449.5115314140435, K-means Loss: 248.32533264160156\n",
      "Epoch 479\n",
      "Reconstruction Loss: 3448.1593890017566, K-means Loss: 248.1398468017578\n",
      "Epoch 480\n",
      "Reconstruction Loss: 3445.839368379697, K-means Loss: 249.00665283203125\n",
      "Epoch 481\n",
      "Reconstruction Loss: 3444.4812950344585, K-means Loss: 248.7841339111328\n",
      "Epoch 482\n",
      "Reconstruction Loss: 3442.234870666692, K-means Loss: 249.54400634765625\n",
      "Epoch 483\n",
      "Reconstruction Loss: 3440.9248680931273, K-means Loss: 249.267333984375\n",
      "Epoch 484\n",
      "Reconstruction Loss: 3438.705623119372, K-means Loss: 249.97769165039062\n",
      "Epoch 485\n",
      "Reconstruction Loss: 3437.3704223018135, K-means Loss: 249.6998748779297\n",
      "Epoch 486\n",
      "Reconstruction Loss: 3435.115965869908, K-means Loss: 250.43577575683594\n",
      "Epoch 487\n",
      "Reconstruction Loss: 3433.6872680121887, K-means Loss: 250.24249267578125\n",
      "Epoch 488\n",
      "Reconstruction Loss: 3431.347592356502, K-means Loss: 251.04827880859375\n",
      "Epoch 489\n",
      "Reconstruction Loss: 3429.8566675833004, K-means Loss: 250.91317749023438\n",
      "Epoch 490\n",
      "Reconstruction Loss: 3427.4764744568492, K-means Loss: 251.7412872314453\n",
      "Epoch 491\n",
      "Reconstruction Loss: 3425.9663201830567, K-means Loss: 251.61602783203125\n",
      "Epoch 492\n",
      "Reconstruction Loss: 3423.6013022626357, K-means Loss: 252.3866729736328\n",
      "Epoch 493\n",
      "Reconstruction Loss: 3422.0331334390835, K-means Loss: 252.3221893310547\n",
      "Epoch 494\n",
      "Reconstruction Loss: 3419.5635640031414, K-means Loss: 253.20123291015625\n",
      "Epoch 495\n",
      "Reconstruction Loss: 3417.8489844252526, K-means Loss: 253.29248046875\n",
      "Epoch 496\n",
      "Reconstruction Loss: 3415.3594480005577, K-means Loss: 254.1974334716797\n",
      "Epoch 497\n",
      "Reconstruction Loss: 3413.5683273386903, K-means Loss: 254.33319091796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 498\n",
      "Reconstruction Loss: 3411.0857524488956, K-means Loss: 255.1839599609375\n",
      "Epoch 499\n",
      "Reconstruction Loss: 3409.2872215567477, K-means Loss: 255.3475341796875\n",
      "Epoch 500\n",
      "Reconstruction Loss: 3406.896756344788, K-means Loss: 256.1614685058594\n",
      "Epoch 501\n",
      "Reconstruction Loss: 3405.0260244605633, K-means Loss: 256.3887634277344\n",
      "Epoch 502\n",
      "Reconstruction Loss: 3402.6871407521344, K-means Loss: 257.1556396484375\n",
      "Epoch 503\n",
      "Reconstruction Loss: 3400.886588479497, K-means Loss: 257.33856201171875\n",
      "Epoch 504\n",
      "Reconstruction Loss: 3398.6729611419346, K-means Loss: 257.98175048828125\n",
      "Epoch 505\n",
      "Reconstruction Loss: 3396.9025619945423, K-means Loss: 258.1225891113281\n",
      "Epoch 506\n",
      "Reconstruction Loss: 3394.684419451223, K-means Loss: 258.76678466796875\n",
      "Epoch 507\n",
      "Reconstruction Loss: 3392.8195185566083, K-means Loss: 259.0220947265625\n",
      "Epoch 508\n",
      "Reconstruction Loss: 3390.486224123598, K-means Loss: 259.7745666503906\n",
      "Epoch 509\n",
      "Reconstruction Loss: 3388.3829862940515, K-means Loss: 260.2344665527344\n",
      "Epoch 510\n",
      "Reconstruction Loss: 3385.862919890812, K-means Loss: 261.11029052734375\n",
      "Epoch 511\n",
      "Reconstruction Loss: 3383.540573166228, K-means Loss: 261.66552734375\n",
      "Epoch 512\n",
      "Reconstruction Loss: 3380.6975495751226, K-means Loss: 262.6591491699219\n",
      "Epoch 513\n",
      "Reconstruction Loss: 3378.042291242848, K-means Loss: 263.24456787109375\n",
      "Epoch 514\n",
      "Reconstruction Loss: 3374.945964719684, K-means Loss: 264.38592529296875\n",
      "Epoch 515\n",
      "Reconstruction Loss: 3372.0887118044575, K-means Loss: 265.4057312011719\n",
      "Epoch 516\n",
      "Reconstruction Loss: 3369.1323601932722, K-means Loss: 266.5336608886719\n",
      "Epoch 517\n",
      "Reconstruction Loss: 3366.555896024757, K-means Loss: 267.3168029785156\n",
      "Epoch 518\n",
      "Reconstruction Loss: 3364.1036997264882, K-means Loss: 267.98944091796875\n",
      "Epoch 519\n",
      "Reconstruction Loss: 3362.093067167489, K-means Loss: 268.24212646484375\n",
      "Epoch 520\n",
      "Reconstruction Loss: 3360.0351311605577, K-means Loss: 268.56817626953125\n",
      "Epoch 521\n",
      "Reconstruction Loss: 3358.2713073297114, K-means Loss: 268.6419372558594\n",
      "Epoch 522\n",
      "Reconstruction Loss: 3356.23953373281, K-means Loss: 269.0137634277344\n",
      "Epoch 523\n",
      "Reconstruction Loss: 3354.3416623123408, K-means Loss: 269.24163818359375\n",
      "Epoch 524\n",
      "Reconstruction Loss: 3352.0588493454948, K-means Loss: 269.8493957519531\n",
      "Epoch 525\n",
      "Reconstruction Loss: 3349.877290641671, K-means Loss: 270.3303527832031\n",
      "Epoch 526\n",
      "Reconstruction Loss: 3347.4132372697313, K-means Loss: 271.0968017578125\n",
      "Epoch 527\n",
      "Reconstruction Loss: 3345.149285937656, K-means Loss: 271.6656188964844\n",
      "Epoch 528\n",
      "Reconstruction Loss: 3342.517874089886, K-means Loss: 272.6141357421875\n",
      "Epoch 529\n",
      "Reconstruction Loss: 3340.137076320134, K-means Loss: 273.3072204589844\n",
      "Epoch 530\n",
      "Reconstruction Loss: 3337.626855294258, K-means Loss: 274.1230163574219\n",
      "Epoch 531\n",
      "Reconstruction Loss: 3335.4930141937716, K-means Loss: 274.57073974609375\n",
      "Epoch 532\n",
      "Reconstruction Loss: 3333.155425735896, K-means Loss: 275.21112060546875\n",
      "Epoch 533\n",
      "Reconstruction Loss: 3331.1967489395984, K-means Loss: 275.48504638671875\n",
      "Epoch 534\n",
      "Reconstruction Loss: 3328.7588734933483, K-means Loss: 276.24163818359375\n",
      "Epoch 535\n",
      "Reconstruction Loss: 3326.666264406206, K-means Loss: 276.6706848144531\n",
      "Epoch 536\n",
      "Reconstruction Loss: 3323.944106372766, K-means Loss: 277.7430725097656\n",
      "Epoch 537\n",
      "Reconstruction Loss: 3321.745328545595, K-means Loss: 278.2882995605469\n",
      "Epoch 538\n",
      "Reconstruction Loss: 3319.0633925613747, K-means Loss: 279.2699279785156\n",
      "Epoch 539\n",
      "Reconstruction Loss: 3316.8841479120606, K-means Loss: 279.7375183105469\n",
      "Epoch 540\n",
      "Reconstruction Loss: 3313.9613138886984, K-means Loss: 280.8918762207031\n",
      "Epoch 541\n",
      "Reconstruction Loss: 3311.478554617287, K-means Loss: 281.542236328125\n",
      "Epoch 542\n",
      "Reconstruction Loss: 3308.1732539381787, K-means Loss: 282.8356628417969\n",
      "Epoch 543\n",
      "Reconstruction Loss: 3305.3971602016727, K-means Loss: 283.37713623046875\n",
      "Epoch 544\n",
      "Reconstruction Loss: 3301.7414621357634, K-means Loss: 284.6077575683594\n",
      "Epoch 545\n",
      "Reconstruction Loss: 3298.648147988571, K-means Loss: 285.3862609863281\n",
      "Epoch 546\n",
      "Reconstruction Loss: 3294.3595763857593, K-means Loss: 287.3990783691406\n",
      "Epoch 547\n",
      "Reconstruction Loss: 3291.2615247398726, K-means Loss: 288.1810607910156\n",
      "Epoch 548\n",
      "Reconstruction Loss: 3287.1921042939757, K-means Loss: 290.15545654296875\n",
      "Epoch 549\n",
      "Reconstruction Loss: 3284.688308412954, K-means Loss: 290.67596435546875\n",
      "Epoch 550\n",
      "Reconstruction Loss: 3281.1572564154185, K-means Loss: 292.37249755859375\n",
      "Epoch 551\n",
      "Reconstruction Loss: 3279.3029039857825, K-means Loss: 292.5019836425781\n",
      "Epoch 552\n",
      "Reconstruction Loss: 3276.4364907224754, K-means Loss: 293.80072021484375\n",
      "Epoch 553\n",
      "Reconstruction Loss: 3275.216126260914, K-means Loss: 293.5690612792969\n",
      "Epoch 554\n",
      "Reconstruction Loss: 3272.5139388157268, K-means Loss: 294.9042053222656\n",
      "Epoch 555\n",
      "Reconstruction Loss: 3271.55110516828, K-means Loss: 294.4764709472656\n",
      "Epoch 556\n",
      "Reconstruction Loss: 3268.9576524766603, K-means Loss: 295.6806945800781\n",
      "Epoch 557\n",
      "Reconstruction Loss: 3268.147086365042, K-means Loss: 295.08331298828125\n",
      "Epoch 558\n",
      "Reconstruction Loss: 3265.505308804217, K-means Loss: 296.30096435546875\n",
      "Epoch 559\n",
      "Reconstruction Loss: 3264.5157563289235, K-means Loss: 295.8153991699219\n",
      "Epoch 560\n",
      "Reconstruction Loss: 3261.57284582925, K-means Loss: 297.1821594238281\n",
      "Epoch 561\n",
      "Reconstruction Loss: 3260.6253746020634, K-means Loss: 296.46612548828125\n",
      "Epoch 562\n",
      "Reconstruction Loss: 3257.3989537130415, K-means Loss: 297.89398193359375\n",
      "Epoch 563\n",
      "Reconstruction Loss: 3256.502914789721, K-means Loss: 296.8963623046875\n",
      "Epoch 564\n",
      "Reconstruction Loss: 3253.4319386820366, K-means Loss: 298.05548095703125\n",
      "Epoch 565\n",
      "Reconstruction Loss: 3252.476490404375, K-means Loss: 297.0433654785156\n",
      "Epoch 566\n",
      "Reconstruction Loss: 3249.487476239835, K-means Loss: 298.07965087890625\n",
      "Epoch 567\n",
      "Reconstruction Loss: 3248.4010885618254, K-means Loss: 297.1647033691406\n",
      "Epoch 568\n",
      "Reconstruction Loss: 3245.5063514519456, K-means Loss: 298.0395812988281\n",
      "Epoch 569\n",
      "Reconstruction Loss: 3244.356356748217, K-means Loss: 297.09539794921875\n",
      "Epoch 570\n",
      "Reconstruction Loss: 3241.626914321846, K-means Loss: 297.72296142578125\n",
      "Epoch 571\n",
      "Reconstruction Loss: 3240.446179326661, K-means Loss: 296.7967834472656\n",
      "Epoch 572\n",
      "Reconstruction Loss: 3237.8171653227328, K-means Loss: 297.31640625\n",
      "Epoch 573\n",
      "Reconstruction Loss: 3236.634011708062, K-means Loss: 296.4828796386719\n",
      "Epoch 574\n",
      "Reconstruction Loss: 3234.090476388455, K-means Loss: 296.97430419921875\n",
      "Epoch 575\n",
      "Reconstruction Loss: 3232.7453087437907, K-means Loss: 296.3267517089844\n",
      "Epoch 576\n",
      "Reconstruction Loss: 3230.3791076873745, K-means Loss: 296.73846435546875\n",
      "Epoch 577\n",
      "Reconstruction Loss: 3228.9591050898675, K-means Loss: 296.20916748046875\n",
      "Epoch 578\n",
      "Reconstruction Loss: 3226.5705445923927, K-means Loss: 296.72491455078125\n",
      "Epoch 579\n",
      "Reconstruction Loss: 3225.1174689731706, K-means Loss: 296.2900085449219\n",
      "Epoch 580\n",
      "Reconstruction Loss: 3222.7448165916685, K-means Loss: 296.82586669921875\n",
      "Epoch 581\n",
      "Reconstruction Loss: 3221.3533008388877, K-means Loss: 296.3955383300781\n",
      "Epoch 582\n",
      "Reconstruction Loss: 3218.954167193094, K-means Loss: 296.9922180175781\n",
      "Epoch 583\n",
      "Reconstruction Loss: 3217.611913850059, K-means Loss: 296.5604553222656\n",
      "Epoch 584\n",
      "Reconstruction Loss: 3215.224440026372, K-means Loss: 297.2012939453125\n",
      "Epoch 585\n",
      "Reconstruction Loss: 3213.916782219504, K-means Loss: 296.77056884765625\n",
      "Epoch 586\n",
      "Reconstruction Loss: 3211.508965422737, K-means Loss: 297.4396057128906\n",
      "Epoch 587\n",
      "Reconstruction Loss: 3210.1725629300727, K-means Loss: 297.08270263671875\n",
      "Epoch 588\n",
      "Reconstruction Loss: 3207.7244228807435, K-means Loss: 297.8370056152344\n",
      "Epoch 589\n",
      "Reconstruction Loss: 3206.5297356461124, K-means Loss: 297.3746032714844\n",
      "Epoch 590\n",
      "Reconstruction Loss: 3204.042636366717, K-means Loss: 298.1549072265625\n",
      "Epoch 591\n",
      "Reconstruction Loss: 3202.7325828634957, K-means Loss: 297.7752990722656\n",
      "Epoch 592\n",
      "Reconstruction Loss: 3200.354334832299, K-means Loss: 298.44219970703125\n",
      "Epoch 593\n",
      "Reconstruction Loss: 3199.1040368243735, K-means Loss: 297.9961853027344\n",
      "Epoch 594\n",
      "Reconstruction Loss: 3196.7988470802684, K-means Loss: 298.6031799316406\n",
      "Epoch 595\n",
      "Reconstruction Loss: 3195.5937126412205, K-means Loss: 298.1815185546875\n",
      "Epoch 596\n",
      "Reconstruction Loss: 3193.3148426921507, K-means Loss: 298.84490966796875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 597\n",
      "Reconstruction Loss: 3192.228329310033, K-means Loss: 298.35186767578125\n",
      "Epoch 598\n",
      "Reconstruction Loss: 3189.800125665618, K-means Loss: 299.1777038574219\n",
      "Epoch 599\n",
      "Reconstruction Loss: 3188.716235371192, K-means Loss: 298.6969299316406\n",
      "Epoch 600\n",
      "Reconstruction Loss: 3186.1946293499323, K-means Loss: 299.5812072753906\n",
      "Epoch 601\n",
      "Reconstruction Loss: 3185.153441303385, K-means Loss: 298.95745849609375\n",
      "Epoch 602\n",
      "Reconstruction Loss: 3182.5891583485045, K-means Loss: 299.93890380859375\n",
      "Epoch 603\n",
      "Reconstruction Loss: 3181.7949342762395, K-means Loss: 299.23358154296875\n",
      "Epoch 604\n",
      "Reconstruction Loss: 3179.2997132949654, K-means Loss: 300.2451477050781\n",
      "Epoch 605\n",
      "Reconstruction Loss: 3178.7452929522924, K-means Loss: 299.4448547363281\n",
      "Epoch 606\n",
      "Reconstruction Loss: 3175.879267404725, K-means Loss: 300.9691162109375\n",
      "Epoch 607\n",
      "Reconstruction Loss: 3175.401918998702, K-means Loss: 300.1918029785156\n",
      "Epoch 608\n",
      "Reconstruction Loss: 3172.379279868936, K-means Loss: 301.95367431640625\n",
      "Epoch 609\n",
      "Reconstruction Loss: 3172.082011410274, K-means Loss: 301.1023864746094\n",
      "Epoch 610\n",
      "Reconstruction Loss: 3168.909524978556, K-means Loss: 303.1300354003906\n",
      "Epoch 611\n",
      "Reconstruction Loss: 3168.913588385276, K-means Loss: 302.16705322265625\n",
      "Epoch 612\n",
      "Reconstruction Loss: 3165.4503682483487, K-means Loss: 304.73907470703125\n",
      "Epoch 613\n",
      "Reconstruction Loss: 3165.931872900146, K-means Loss: 303.5553283691406\n",
      "Epoch 614\n",
      "Reconstruction Loss: 3162.207321721, K-means Loss: 306.4183654785156\n",
      "Epoch 615\n",
      "Reconstruction Loss: 3162.7613816146463, K-means Loss: 304.99554443359375\n",
      "Epoch 616\n",
      "Reconstruction Loss: 3158.6146286102994, K-means Loss: 308.08050537109375\n",
      "Epoch 617\n",
      "Reconstruction Loss: 3159.1701096110933, K-means Loss: 306.4761047363281\n",
      "Epoch 618\n",
      "Reconstruction Loss: 3154.8853146851798, K-means Loss: 309.5289611816406\n",
      "Epoch 619\n",
      "Reconstruction Loss: 3155.1809632291747, K-means Loss: 307.7853088378906\n",
      "Epoch 620\n",
      "Reconstruction Loss: 3150.3027847585354, K-means Loss: 311.3073425292969\n",
      "Epoch 621\n",
      "Reconstruction Loss: 3150.4775279239384, K-means Loss: 309.6329345703125\n",
      "Epoch 622\n",
      "Reconstruction Loss: 3145.376533674151, K-means Loss: 313.03118896484375\n",
      "Epoch 623\n",
      "Reconstruction Loss: 3145.32834335299, K-means Loss: 311.20660400390625\n",
      "Epoch 624\n",
      "Reconstruction Loss: 3140.0541902095974, K-means Loss: 314.51385498046875\n",
      "Epoch 625\n",
      "Reconstruction Loss: 3139.6645039930763, K-means Loss: 312.9235534667969\n",
      "Epoch 626\n",
      "Reconstruction Loss: 3134.454104637284, K-means Loss: 316.22076416015625\n",
      "Epoch 627\n",
      "Reconstruction Loss: 3134.1193215392013, K-means Loss: 314.85357666015625\n",
      "Epoch 628\n",
      "Reconstruction Loss: 3129.230374640913, K-means Loss: 318.13201904296875\n",
      "Epoch 629\n",
      "Reconstruction Loss: 3129.0677357055665, K-means Loss: 316.5887756347656\n",
      "Epoch 630\n",
      "Reconstruction Loss: 3124.428404811695, K-means Loss: 319.29364013671875\n",
      "Epoch 631\n",
      "Reconstruction Loss: 3124.4125756215058, K-means Loss: 317.12744140625\n",
      "Epoch 632\n",
      "Reconstruction Loss: 3120.266882844217, K-means Loss: 318.8323059082031\n",
      "Epoch 633\n",
      "Reconstruction Loss: 3120.1758316933747, K-means Loss: 316.4394836425781\n",
      "Epoch 634\n",
      "Reconstruction Loss: 3116.468423948035, K-means Loss: 317.591064453125\n",
      "Epoch 635\n",
      "Reconstruction Loss: 3116.2924567177015, K-means Loss: 315.16754150390625\n",
      "Epoch 636\n",
      "Reconstruction Loss: 3112.9459181954453, K-means Loss: 315.9396057128906\n",
      "Epoch 637\n",
      "Reconstruction Loss: 3112.710019540219, K-means Loss: 313.710205078125\n",
      "Epoch 638\n",
      "Reconstruction Loss: 3109.5217139465317, K-means Loss: 314.51080322265625\n",
      "Epoch 639\n",
      "Reconstruction Loss: 3109.1475383095094, K-means Loss: 312.6488037109375\n",
      "Epoch 640\n",
      "Reconstruction Loss: 3106.0305757848505, K-means Loss: 313.5740661621094\n",
      "Epoch 641\n",
      "Reconstruction Loss: 3105.4537186662474, K-means Loss: 312.0570373535156\n",
      "Epoch 642\n",
      "Reconstruction Loss: 3102.391474740043, K-means Loss: 313.1411437988281\n",
      "Epoch 643\n",
      "Reconstruction Loss: 3101.8050635250625, K-means Loss: 311.8172302246094\n",
      "Epoch 644\n",
      "Reconstruction Loss: 3098.818149628268, K-means Loss: 312.9505920410156\n",
      "Epoch 645\n",
      "Reconstruction Loss: 3098.2737578651922, K-means Loss: 311.6942138671875\n",
      "Epoch 646\n",
      "Reconstruction Loss: 3095.2857712571617, K-means Loss: 312.9497375488281\n",
      "Epoch 647\n",
      "Reconstruction Loss: 3094.8816727781477, K-means Loss: 311.7181396484375\n",
      "Epoch 648\n",
      "Reconstruction Loss: 3091.9262980771273, K-means Loss: 313.1234436035156\n",
      "Epoch 649\n",
      "Reconstruction Loss: 3091.681899580605, K-means Loss: 311.9474792480469\n",
      "Epoch 650\n",
      "Reconstruction Loss: 3088.6967941823336, K-means Loss: 313.656982421875\n",
      "Epoch 651\n",
      "Reconstruction Loss: 3088.7599547744244, K-means Loss: 312.4224853515625\n",
      "Epoch 652\n",
      "Reconstruction Loss: 3085.509240359152, K-means Loss: 314.6882629394531\n",
      "Epoch 653\n",
      "Reconstruction Loss: 3085.777496681553, K-means Loss: 313.57647705078125\n",
      "Epoch 654\n",
      "Reconstruction Loss: 3082.3537565831202, K-means Loss: 316.24713134765625\n",
      "Epoch 655\n",
      "Reconstruction Loss: 3082.8283502557033, K-means Loss: 315.04278564453125\n",
      "Epoch 656\n",
      "Reconstruction Loss: 3079.1394336861576, K-means Loss: 317.9329833984375\n",
      "Epoch 657\n",
      "Reconstruction Loss: 3079.8101736703557, K-means Loss: 316.339111328125\n",
      "Epoch 658\n",
      "Reconstruction Loss: 3075.8551173601195, K-means Loss: 319.1573791503906\n",
      "Epoch 659\n",
      "Reconstruction Loss: 3076.6555606134507, K-means Loss: 317.1398010253906\n",
      "Epoch 660\n",
      "Reconstruction Loss: 3072.5954672661624, K-means Loss: 319.6401062011719\n",
      "Epoch 661\n",
      "Reconstruction Loss: 3073.2618570764225, K-means Loss: 317.2290344238281\n",
      "Epoch 662\n",
      "Reconstruction Loss: 3069.380732394837, K-means Loss: 319.12677001953125\n",
      "Epoch 663\n",
      "Reconstruction Loss: 3069.840331137393, K-means Loss: 316.6754455566406\n",
      "Epoch 664\n",
      "Reconstruction Loss: 3066.0514039557625, K-means Loss: 318.4255065917969\n",
      "Epoch 665\n",
      "Reconstruction Loss: 3066.319387312501, K-means Loss: 316.19696044921875\n",
      "Epoch 666\n",
      "Reconstruction Loss: 3062.7693648435384, K-means Loss: 317.6846923828125\n",
      "Epoch 667\n",
      "Reconstruction Loss: 3062.8538943371154, K-means Loss: 315.6692199707031\n",
      "Epoch 668\n",
      "Reconstruction Loss: 3059.5150816433643, K-means Loss: 317.1002197265625\n",
      "Epoch 669\n",
      "Reconstruction Loss: 3059.453601260756, K-means Loss: 315.4728698730469\n",
      "Epoch 670\n",
      "Reconstruction Loss: 3056.34055916474, K-means Loss: 316.9025573730469\n",
      "Epoch 671\n",
      "Reconstruction Loss: 3056.3059787978855, K-means Loss: 315.4579772949219\n",
      "Epoch 672\n",
      "Reconstruction Loss: 3053.2090095407666, K-means Loss: 317.0364074707031\n",
      "Epoch 673\n",
      "Reconstruction Loss: 3053.1563320566215, K-means Loss: 315.7408752441406\n",
      "Epoch 674\n",
      "Reconstruction Loss: 3050.131054540003, K-means Loss: 317.3798828125\n",
      "Epoch 675\n",
      "Reconstruction Loss: 3050.183247540874, K-means Loss: 316.10064697265625\n",
      "Epoch 676\n",
      "Reconstruction Loss: 3047.1566189847185, K-means Loss: 317.88433837890625\n",
      "Epoch 677\n",
      "Reconstruction Loss: 3047.324989561329, K-means Loss: 316.62518310546875\n",
      "Epoch 678\n",
      "Reconstruction Loss: 3044.206198559077, K-means Loss: 318.5597839355469\n",
      "Epoch 679\n",
      "Reconstruction Loss: 3044.313768296443, K-means Loss: 317.341064453125\n",
      "Epoch 680\n",
      "Reconstruction Loss: 3041.1876512373356, K-means Loss: 319.29736328125\n",
      "Epoch 681\n",
      "Reconstruction Loss: 3041.3536742524284, K-means Loss: 318.0750427246094\n",
      "Epoch 682\n",
      "Reconstruction Loss: 3038.06054055366, K-means Loss: 320.2929382324219\n",
      "Epoch 683\n",
      "Reconstruction Loss: 3038.370860483571, K-means Loss: 318.9908752441406\n",
      "Epoch 684\n",
      "Reconstruction Loss: 3034.9768349209953, K-means Loss: 321.32177734375\n",
      "Epoch 685\n",
      "Reconstruction Loss: 3035.492454942846, K-means Loss: 319.76556396484375\n",
      "Epoch 686\n",
      "Reconstruction Loss: 3031.97665336718, K-means Loss: 322.08642578125\n",
      "Epoch 687\n",
      "Reconstruction Loss: 3032.542961567763, K-means Loss: 320.2489929199219\n",
      "Epoch 688\n",
      "Reconstruction Loss: 3028.8597876481626, K-means Loss: 322.5199279785156\n",
      "Epoch 689\n",
      "Reconstruction Loss: 3029.3851171515344, K-means Loss: 320.68426513671875\n",
      "Epoch 690\n",
      "Reconstruction Loss: 3025.596725468011, K-means Loss: 322.99420166015625\n",
      "Epoch 691\n",
      "Reconstruction Loss: 3026.087632546261, K-means Loss: 321.1094665527344\n",
      "Epoch 692\n",
      "Reconstruction Loss: 3022.2996756074926, K-means Loss: 323.3844299316406\n",
      "Epoch 693\n",
      "Reconstruction Loss: 3022.8865315416115, K-means Loss: 321.4280090332031\n",
      "Epoch 694\n",
      "Reconstruction Loss: 3019.0449738835428, K-means Loss: 323.85052490234375\n",
      "Epoch 695\n",
      "Reconstruction Loss: 3019.5596156708593, K-means Loss: 321.9055480957031\n",
      "Epoch 696\n",
      "Reconstruction Loss: 3015.467391061417, K-means Loss: 324.5240478515625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 697\n",
      "Reconstruction Loss: 3016.160455011771, K-means Loss: 322.4096374511719\n",
      "Epoch 698\n",
      "Reconstruction Loss: 3011.9679999933082, K-means Loss: 325.103759765625\n",
      "Epoch 699\n",
      "Reconstruction Loss: 3012.5251826915246, K-means Loss: 323.14019775390625\n",
      "Epoch 700\n",
      "Reconstruction Loss: 3008.0592790008895, K-means Loss: 326.0894470214844\n",
      "Epoch 701\n",
      "Reconstruction Loss: 3008.7088599100925, K-means Loss: 323.96771240234375\n",
      "Epoch 702\n",
      "Reconstruction Loss: 3004.331206952564, K-means Loss: 326.66741943359375\n",
      "Epoch 703\n",
      "Reconstruction Loss: 3004.82548740011, K-means Loss: 324.6186828613281\n",
      "Epoch 704\n",
      "Reconstruction Loss: 3000.6226037548136, K-means Loss: 327.3043518066406\n",
      "Epoch 705\n",
      "Reconstruction Loss: 3001.327613131182, K-means Loss: 325.13189697265625\n",
      "Epoch 706\n",
      "Reconstruction Loss: 2997.368324745397, K-means Loss: 327.644287109375\n",
      "Epoch 707\n",
      "Reconstruction Loss: 2998.295379854503, K-means Loss: 325.35394287109375\n",
      "Epoch 708\n",
      "Reconstruction Loss: 2994.359201113939, K-means Loss: 327.89263916015625\n",
      "Epoch 709\n",
      "Reconstruction Loss: 2995.365628002544, K-means Loss: 325.4948425292969\n",
      "Epoch 710\n",
      "Reconstruction Loss: 2991.3599941832945, K-means Loss: 328.0773010253906\n",
      "Epoch 711\n",
      "Reconstruction Loss: 2992.3215795309666, K-means Loss: 325.7788391113281\n",
      "Epoch 712\n",
      "Reconstruction Loss: 2988.386235851746, K-means Loss: 328.35040283203125\n",
      "Epoch 713\n",
      "Reconstruction Loss: 2989.4206829246436, K-means Loss: 325.96826171875\n",
      "Epoch 714\n",
      "Reconstruction Loss: 2985.4929966968803, K-means Loss: 328.479248046875\n",
      "Epoch 715\n",
      "Reconstruction Loss: 2986.546600727408, K-means Loss: 326.0573425292969\n",
      "Epoch 716\n",
      "Reconstruction Loss: 2982.7661181689737, K-means Loss: 328.49017333984375\n",
      "Epoch 717\n",
      "Reconstruction Loss: 2983.9631403878866, K-means Loss: 326.0167236328125\n",
      "Epoch 718\n",
      "Reconstruction Loss: 2980.070488541982, K-means Loss: 328.6250305175781\n",
      "Epoch 719\n",
      "Reconstruction Loss: 2981.302557772722, K-means Loss: 326.1365661621094\n",
      "Epoch 720\n",
      "Reconstruction Loss: 2977.377306402318, K-means Loss: 328.8033752441406\n",
      "Epoch 721\n",
      "Reconstruction Loss: 2978.5317548704775, K-means Loss: 326.3919677734375\n",
      "Epoch 722\n",
      "Reconstruction Loss: 2974.585978411602, K-means Loss: 329.09521484375\n",
      "Epoch 723\n",
      "Reconstruction Loss: 2975.832050786194, K-means Loss: 326.6573181152344\n",
      "Epoch 724\n",
      "Reconstruction Loss: 2971.8745588564243, K-means Loss: 329.4022521972656\n",
      "Epoch 725\n",
      "Reconstruction Loss: 2973.219671696665, K-means Loss: 326.8898620605469\n",
      "Epoch 726\n",
      "Reconstruction Loss: 2969.2299945150176, K-means Loss: 329.5810852050781\n",
      "Epoch 727\n",
      "Reconstruction Loss: 2970.5240973421046, K-means Loss: 327.0184631347656\n",
      "Epoch 728\n",
      "Reconstruction Loss: 2966.62545394225, K-means Loss: 329.57794189453125\n",
      "Epoch 729\n",
      "Reconstruction Loss: 2967.9623438864483, K-means Loss: 326.9935607910156\n",
      "Epoch 730\n",
      "Reconstruction Loss: 2963.9957735020967, K-means Loss: 329.6782531738281\n",
      "Epoch 731\n",
      "Reconstruction Loss: 2965.3468535523034, K-means Loss: 327.11700439453125\n",
      "Epoch 732\n",
      "Reconstruction Loss: 2961.3773753715827, K-means Loss: 329.7884521484375\n",
      "Epoch 733\n",
      "Reconstruction Loss: 2962.702602301172, K-means Loss: 327.25701904296875\n",
      "Epoch 734\n",
      "Reconstruction Loss: 2958.742360918372, K-means Loss: 329.9657897949219\n",
      "Epoch 735\n",
      "Reconstruction Loss: 2960.066919615228, K-means Loss: 327.4044189453125\n",
      "Epoch 736\n",
      "Reconstruction Loss: 2956.105690624073, K-means Loss: 330.0750427246094\n",
      "Epoch 737\n",
      "Reconstruction Loss: 2957.380628787503, K-means Loss: 327.5514831542969\n",
      "Epoch 738\n",
      "Reconstruction Loss: 2953.48054188284, K-means Loss: 330.1642150878906\n",
      "Epoch 739\n",
      "Reconstruction Loss: 2954.823880905626, K-means Loss: 327.6075744628906\n",
      "Epoch 740\n",
      "Reconstruction Loss: 2950.9110214377265, K-means Loss: 330.2278137207031\n",
      "Epoch 741\n",
      "Reconstruction Loss: 2952.3081285060985, K-means Loss: 327.6521911621094\n",
      "Epoch 742\n",
      "Reconstruction Loss: 2948.42018124787, K-means Loss: 330.2527770996094\n",
      "Epoch 743\n",
      "Reconstruction Loss: 2949.815018156511, K-means Loss: 327.6793212890625\n",
      "Epoch 744\n",
      "Reconstruction Loss: 2945.8592945804435, K-means Loss: 330.3775939941406\n",
      "Epoch 745\n",
      "Reconstruction Loss: 2947.201736696, K-means Loss: 327.8572998046875\n",
      "Epoch 746\n",
      "Reconstruction Loss: 2943.2113719014533, K-means Loss: 330.6044006347656\n",
      "Epoch 747\n",
      "Reconstruction Loss: 2944.597980762795, K-means Loss: 328.0776672363281\n",
      "Epoch 748\n",
      "Reconstruction Loss: 2940.5923956603933, K-means Loss: 330.8393249511719\n",
      "Epoch 749\n",
      "Reconstruction Loss: 2942.116723086915, K-means Loss: 328.16302490234375\n",
      "Epoch 750\n",
      "Reconstruction Loss: 2938.0274360096464, K-means Loss: 331.0180969238281\n",
      "Epoch 751\n",
      "Reconstruction Loss: 2939.70107185125, K-means Loss: 328.1860656738281\n",
      "Epoch 752\n",
      "Reconstruction Loss: 2935.6630996564722, K-means Loss: 330.9848937988281\n",
      "Epoch 753\n",
      "Reconstruction Loss: 2937.2612246553404, K-means Loss: 328.24896240234375\n",
      "Epoch 754\n",
      "Reconstruction Loss: 2933.188951490192, K-means Loss: 331.0199890136719\n",
      "Epoch 755\n",
      "Reconstruction Loss: 2934.6925482294173, K-means Loss: 328.2480773925781\n",
      "Epoch 756\n",
      "Reconstruction Loss: 2930.607239569829, K-means Loss: 330.88824462890625\n",
      "Epoch 757\n",
      "Reconstruction Loss: 2932.012044946892, K-means Loss: 328.15576171875\n",
      "Epoch 758\n",
      "Reconstruction Loss: 2928.064807872085, K-means Loss: 330.6808166503906\n",
      "Epoch 759\n",
      "Reconstruction Loss: 2929.442141378258, K-means Loss: 327.9703674316406\n",
      "Epoch 760\n",
      "Reconstruction Loss: 2925.58724947791, K-means Loss: 330.366943359375\n",
      "Epoch 761\n",
      "Reconstruction Loss: 2926.8460226128595, K-means Loss: 327.71649169921875\n",
      "Epoch 762\n",
      "Reconstruction Loss: 2923.0370488249137, K-means Loss: 330.02685546875\n",
      "Epoch 763\n",
      "Reconstruction Loss: 2924.1311604984817, K-means Loss: 327.51702880859375\n",
      "Epoch 764\n",
      "Reconstruction Loss: 2920.458051680443, K-means Loss: 329.789794921875\n",
      "Epoch 765\n",
      "Reconstruction Loss: 2921.5454011916067, K-means Loss: 327.4151611328125\n",
      "Epoch 766\n",
      "Reconstruction Loss: 2918.000998710603, K-means Loss: 329.6603088378906\n",
      "Epoch 767\n",
      "Reconstruction Loss: 2919.1525363402434, K-means Loss: 327.2690124511719\n",
      "Epoch 768\n",
      "Reconstruction Loss: 2915.671357032675, K-means Loss: 329.5201110839844\n",
      "Epoch 769\n",
      "Reconstruction Loss: 2916.8325427038126, K-means Loss: 327.1716003417969\n",
      "Epoch 770\n",
      "Reconstruction Loss: 2913.3171893821195, K-means Loss: 329.49969482421875\n",
      "Epoch 771\n",
      "Reconstruction Loss: 2914.4449715983123, K-means Loss: 327.18585205078125\n",
      "Epoch 772\n",
      "Reconstruction Loss: 2910.9436820393457, K-means Loss: 329.487060546875\n",
      "Epoch 773\n",
      "Reconstruction Loss: 2912.070390213889, K-means Loss: 327.15618896484375\n",
      "Epoch 774\n",
      "Reconstruction Loss: 2908.5352585668497, K-means Loss: 329.4561462402344\n",
      "Epoch 775\n",
      "Reconstruction Loss: 2909.7380639985, K-means Loss: 327.0889587402344\n",
      "Epoch 776\n",
      "Reconstruction Loss: 2906.1631986688767, K-means Loss: 329.4856262207031\n",
      "Epoch 777\n",
      "Reconstruction Loss: 2907.511606086547, K-means Loss: 327.0804443359375\n",
      "Epoch 778\n",
      "Reconstruction Loss: 2903.9300221218614, K-means Loss: 329.5995178222656\n",
      "Epoch 779\n",
      "Reconstruction Loss: 2905.410577573607, K-means Loss: 327.16888427734375\n",
      "Epoch 780\n",
      "Reconstruction Loss: 2901.709199597185, K-means Loss: 329.81146240234375\n",
      "Epoch 781\n",
      "Reconstruction Loss: 2903.2525352253524, K-means Loss: 327.2797546386719\n",
      "Epoch 782\n",
      "Reconstruction Loss: 2899.4349577685293, K-means Loss: 330.0247497558594\n",
      "Epoch 783\n",
      "Reconstruction Loss: 2900.943480538356, K-means Loss: 327.5466613769531\n",
      "Epoch 784\n",
      "Reconstruction Loss: 2897.0834215509262, K-means Loss: 330.3886413574219\n",
      "Epoch 785\n",
      "Reconstruction Loss: 2898.6454865206706, K-means Loss: 327.8677673339844\n",
      "Epoch 786\n",
      "Reconstruction Loss: 2894.6934714763925, K-means Loss: 330.7848815917969\n",
      "Epoch 787\n",
      "Reconstruction Loss: 2896.4968885905205, K-means Loss: 328.02825927734375\n",
      "Epoch 788\n",
      "Reconstruction Loss: 2892.4796693163744, K-means Loss: 330.9516296386719\n",
      "Epoch 789\n",
      "Reconstruction Loss: 2894.225581320657, K-means Loss: 328.2131042480469\n",
      "Epoch 790\n",
      "Reconstruction Loss: 2890.2814082881964, K-means Loss: 331.02178955078125\n",
      "Epoch 791\n",
      "Reconstruction Loss: 2891.947731544626, K-means Loss: 328.3470458984375\n",
      "Epoch 792\n",
      "Reconstruction Loss: 2887.9457306226914, K-means Loss: 331.2575378417969\n",
      "Epoch 793\n",
      "Reconstruction Loss: 2889.71131670438, K-means Loss: 328.5652770996094\n",
      "Epoch 794\n",
      "Reconstruction Loss: 2885.6356494028582, K-means Loss: 331.61676025390625\n",
      "Epoch 795\n",
      "Reconstruction Loss: 2887.563390210753, K-means Loss: 328.8428039550781\n",
      "Epoch 796\n",
      "Reconstruction Loss: 2883.503948911068, K-means Loss: 331.9034423828125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 797\n",
      "Reconstruction Loss: 2885.5013146406914, K-means Loss: 328.99542236328125\n",
      "Epoch 798\n",
      "Reconstruction Loss: 2881.3929652769048, K-means Loss: 332.03314208984375\n",
      "Epoch 799\n",
      "Reconstruction Loss: 2883.339468857054, K-means Loss: 329.1091003417969\n",
      "Epoch 800\n",
      "Reconstruction Loss: 2879.2220626202734, K-means Loss: 332.13995361328125\n",
      "Epoch 801\n",
      "Reconstruction Loss: 2881.16991618441, K-means Loss: 329.1974182128906\n",
      "Epoch 802\n",
      "Reconstruction Loss: 2877.0123244019874, K-means Loss: 332.2646179199219\n",
      "Epoch 803\n",
      "Reconstruction Loss: 2878.957519351007, K-means Loss: 329.26019287109375\n",
      "Epoch 804\n",
      "Reconstruction Loss: 2874.8128454954212, K-means Loss: 332.24658203125\n",
      "Epoch 805\n",
      "Reconstruction Loss: 2876.7556493694187, K-means Loss: 329.2284240722656\n",
      "Epoch 806\n",
      "Reconstruction Loss: 2872.664054062905, K-means Loss: 332.1753845214844\n",
      "Epoch 807\n",
      "Reconstruction Loss: 2874.553553091842, K-means Loss: 329.1670837402344\n",
      "Epoch 808\n",
      "Reconstruction Loss: 2870.504577024742, K-means Loss: 331.9706115722656\n",
      "Epoch 809\n",
      "Reconstruction Loss: 2872.30253916115, K-means Loss: 329.00579833984375\n",
      "Epoch 810\n",
      "Reconstruction Loss: 2868.3457723732167, K-means Loss: 331.7133483886719\n",
      "Epoch 811\n",
      "Reconstruction Loss: 2870.0237987188693, K-means Loss: 328.8912353515625\n",
      "Epoch 812\n",
      "Reconstruction Loss: 2866.1423703466407, K-means Loss: 331.54632568359375\n",
      "Epoch 813\n",
      "Reconstruction Loss: 2867.7281000868134, K-means Loss: 328.8180236816406\n",
      "Epoch 814\n",
      "Reconstruction Loss: 2863.9334954444207, K-means Loss: 331.3603515625\n",
      "Epoch 815\n",
      "Reconstruction Loss: 2865.4580065844693, K-means Loss: 328.6715393066406\n",
      "Epoch 816\n",
      "Reconstruction Loss: 2861.68346311782, K-means Loss: 331.1640625\n",
      "Epoch 817\n",
      "Reconstruction Loss: 2863.163246893641, K-means Loss: 328.5261535644531\n",
      "Epoch 818\n",
      "Reconstruction Loss: 2859.4614097219505, K-means Loss: 330.9231262207031\n",
      "Epoch 819\n",
      "Reconstruction Loss: 2860.8219533637766, K-means Loss: 328.392578125\n",
      "Epoch 820\n",
      "Reconstruction Loss: 2857.2818799640368, K-means Loss: 330.6160583496094\n",
      "Epoch 821\n",
      "Reconstruction Loss: 2858.471699090631, K-means Loss: 328.21539306640625\n",
      "Epoch 822\n",
      "Reconstruction Loss: 2855.079546680158, K-means Loss: 330.2660827636719\n",
      "Epoch 823\n",
      "Reconstruction Loss: 2856.150168594755, K-means Loss: 327.9820251464844\n",
      "Epoch 824\n",
      "Reconstruction Loss: 2852.918907514961, K-means Loss: 329.9107971191406\n",
      "Epoch 825\n",
      "Reconstruction Loss: 2853.9419728405524, K-means Loss: 327.709716796875\n",
      "Epoch 826\n",
      "Reconstruction Loss: 2850.826133989608, K-means Loss: 329.5516052246094\n",
      "Epoch 827\n",
      "Reconstruction Loss: 2851.7835628690923, K-means Loss: 327.4446105957031\n",
      "Epoch 828\n",
      "Reconstruction Loss: 2848.775459407852, K-means Loss: 329.2226867675781\n",
      "Epoch 829\n",
      "Reconstruction Loss: 2849.625466473507, K-means Loss: 327.23126220703125\n",
      "Epoch 830\n",
      "Reconstruction Loss: 2846.661218063221, K-means Loss: 329.0087890625\n",
      "Epoch 831\n",
      "Reconstruction Loss: 2847.437655058824, K-means Loss: 327.12982177734375\n",
      "Epoch 832\n",
      "Reconstruction Loss: 2844.5536745130244, K-means Loss: 328.85040283203125\n",
      "Epoch 833\n",
      "Reconstruction Loss: 2845.322245988229, K-means Loss: 326.9955139160156\n",
      "Epoch 834\n",
      "Reconstruction Loss: 2842.4954842686457, K-means Loss: 328.7115478515625\n",
      "Epoch 835\n",
      "Reconstruction Loss: 2843.343131498978, K-means Loss: 326.83447265625\n",
      "Epoch 836\n",
      "Reconstruction Loss: 2840.5273328343624, K-means Loss: 328.61212158203125\n",
      "Epoch 837\n",
      "Reconstruction Loss: 2841.5069841790446, K-means Loss: 326.74554443359375\n",
      "Epoch 838\n",
      "Reconstruction Loss: 2838.5292191621356, K-means Loss: 328.8310546875\n",
      "Epoch 839\n",
      "Reconstruction Loss: 2839.738996037598, K-means Loss: 326.85015869140625\n",
      "Epoch 840\n",
      "Reconstruction Loss: 2836.495726470653, K-means Loss: 329.2578430175781\n",
      "Epoch 841\n",
      "Reconstruction Loss: 2837.798971930402, K-means Loss: 327.24053955078125\n",
      "Epoch 842\n",
      "Reconstruction Loss: 2834.4401163335892, K-means Loss: 329.8680419921875\n",
      "Epoch 843\n",
      "Reconstruction Loss: 2835.9618432061384, K-means Loss: 327.7158203125\n",
      "Epoch 844\n",
      "Reconstruction Loss: 2832.455339418197, K-means Loss: 330.6293640136719\n",
      "Epoch 845\n",
      "Reconstruction Loss: 2834.320563226354, K-means Loss: 328.2964782714844\n",
      "Epoch 846\n",
      "Reconstruction Loss: 2830.6099682037643, K-means Loss: 331.65997314453125\n",
      "Epoch 847\n",
      "Reconstruction Loss: 2833.0017757346086, K-means Loss: 329.13323974609375\n",
      "Epoch 848\n",
      "Reconstruction Loss: 2828.936485545677, K-means Loss: 333.07928466796875\n",
      "Epoch 849\n",
      "Reconstruction Loss: 2831.8037945872306, K-means Loss: 330.22662353515625\n",
      "Epoch 850\n",
      "Reconstruction Loss: 2827.339550549769, K-means Loss: 334.7564392089844\n",
      "Epoch 851\n",
      "Reconstruction Loss: 2830.7340239112345, K-means Loss: 331.5938720703125\n",
      "Epoch 852\n",
      "Reconstruction Loss: 2825.8416597066484, K-means Loss: 336.56842041015625\n",
      "Epoch 853\n",
      "Reconstruction Loss: 2829.453397859317, K-means Loss: 332.9149475097656\n",
      "Epoch 854\n",
      "Reconstruction Loss: 2824.2223556591634, K-means Loss: 337.9209899902344\n",
      "Epoch 855\n",
      "Reconstruction Loss: 2827.8699494758275, K-means Loss: 333.7689208984375\n",
      "Epoch 856\n",
      "Reconstruction Loss: 2822.349691941815, K-means Loss: 338.4831848144531\n",
      "Epoch 857\n",
      "Reconstruction Loss: 2825.7965649078524, K-means Loss: 333.8533935546875\n",
      "Epoch 858\n",
      "Reconstruction Loss: 2820.05660330633, K-means Loss: 338.062744140625\n",
      "Epoch 859\n",
      "Reconstruction Loss: 2823.1376395142092, K-means Loss: 333.2707214355469\n",
      "Epoch 860\n",
      "Reconstruction Loss: 2817.6137978871334, K-means Loss: 336.89447021484375\n",
      "Epoch 861\n",
      "Reconstruction Loss: 2820.27082651565, K-means Loss: 332.27142333984375\n",
      "Epoch 862\n",
      "Reconstruction Loss: 2815.065980934855, K-means Loss: 335.406982421875\n",
      "Epoch 863\n",
      "Reconstruction Loss: 2817.316036506053, K-means Loss: 331.0989990234375\n",
      "Epoch 864\n",
      "Reconstruction Loss: 2812.619606833335, K-means Loss: 333.6610412597656\n",
      "Epoch 865\n",
      "Reconstruction Loss: 2814.4822340383016, K-means Loss: 329.9244079589844\n",
      "Epoch 866\n",
      "Reconstruction Loss: 2810.363331967082, K-means Loss: 332.1474914550781\n",
      "Epoch 867\n",
      "Reconstruction Loss: 2811.8468883685077, K-means Loss: 328.94708251953125\n",
      "Epoch 868\n",
      "Reconstruction Loss: 2808.2343598688212, K-means Loss: 330.87811279296875\n",
      "Epoch 869\n",
      "Reconstruction Loss: 2809.482430724119, K-means Loss: 328.1203308105469\n",
      "Epoch 870\n",
      "Reconstruction Loss: 2806.1745112079025, K-means Loss: 330.0022277832031\n",
      "Epoch 871\n",
      "Reconstruction Loss: 2807.2116733992816, K-means Loss: 327.6473083496094\n",
      "Epoch 872\n",
      "Reconstruction Loss: 2804.167886866207, K-means Loss: 329.4086608886719\n",
      "Epoch 873\n",
      "Reconstruction Loss: 2804.9930715374326, K-means Loss: 327.3418884277344\n",
      "Epoch 874\n",
      "Reconstruction Loss: 2802.126038026004, K-means Loss: 329.0021667480469\n",
      "Epoch 875\n",
      "Reconstruction Loss: 2802.861035867795, K-means Loss: 327.0903015136719\n",
      "Epoch 876\n",
      "Reconstruction Loss: 2800.2015289395285, K-means Loss: 328.6034851074219\n",
      "Epoch 877\n",
      "Reconstruction Loss: 2800.815058440484, K-means Loss: 326.87982177734375\n",
      "Epoch 878\n",
      "Reconstruction Loss: 2798.342570945519, K-means Loss: 328.2413635253906\n",
      "Epoch 879\n",
      "Reconstruction Loss: 2798.796760923741, K-means Loss: 326.7345886230469\n",
      "Epoch 880\n",
      "Reconstruction Loss: 2796.4900608054118, K-means Loss: 328.01446533203125\n",
      "Epoch 881\n",
      "Reconstruction Loss: 2796.790441208145, K-means Loss: 326.7340393066406\n",
      "Epoch 882\n",
      "Reconstruction Loss: 2794.6417544600595, K-means Loss: 327.904296875\n",
      "Epoch 883\n",
      "Reconstruction Loss: 2794.840657191958, K-means Loss: 326.7420349121094\n",
      "Epoch 884\n",
      "Reconstruction Loss: 2792.7384560538617, K-means Loss: 327.87127685546875\n",
      "Epoch 885\n",
      "Reconstruction Loss: 2792.8754366295634, K-means Loss: 326.78179931640625\n",
      "Epoch 886\n",
      "Reconstruction Loss: 2790.8259415478783, K-means Loss: 327.8786926269531\n",
      "Epoch 887\n",
      "Reconstruction Loss: 2790.9048359422723, K-means Loss: 326.89190673828125\n",
      "Epoch 888\n",
      "Reconstruction Loss: 2788.942479124211, K-means Loss: 327.92938232421875\n",
      "Epoch 889\n",
      "Reconstruction Loss: 2789.0095187273932, K-means Loss: 326.97119140625\n",
      "Epoch 890\n",
      "Reconstruction Loss: 2787.092679084118, K-means Loss: 327.9909362792969\n",
      "Epoch 891\n",
      "Reconstruction Loss: 2787.1681554109277, K-means Loss: 327.0567321777344\n",
      "Epoch 892\n",
      "Reconstruction Loss: 2785.280127098764, K-means Loss: 328.07574462890625\n",
      "Epoch 893\n",
      "Reconstruction Loss: 2785.4126867477767, K-means Loss: 327.08270263671875\n",
      "Epoch 894\n",
      "Reconstruction Loss: 2783.488935638163, K-means Loss: 328.1588439941406\n",
      "Epoch 895\n",
      "Reconstruction Loss: 2783.6936818093136, K-means Loss: 327.14166259765625\n",
      "Epoch 896\n",
      "Reconstruction Loss: 2781.7289716347977, K-means Loss: 328.32293701171875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 897\n",
      "Reconstruction Loss: 2781.9893117828665, K-means Loss: 327.2845764160156\n",
      "Epoch 898\n",
      "Reconstruction Loss: 2779.9363392308733, K-means Loss: 328.580810546875\n",
      "Epoch 899\n",
      "Reconstruction Loss: 2780.355424082461, K-means Loss: 327.4367370605469\n",
      "Epoch 900\n",
      "Reconstruction Loss: 2778.098683087132, K-means Loss: 328.99090576171875\n",
      "Epoch 901\n",
      "Reconstruction Loss: 2778.768456286996, K-means Loss: 327.649658203125\n",
      "Epoch 902\n",
      "Reconstruction Loss: 2776.245433315824, K-means Loss: 329.5419616699219\n",
      "Epoch 903\n",
      "Reconstruction Loss: 2777.273940449748, K-means Loss: 327.95843505859375\n",
      "Epoch 904\n",
      "Reconstruction Loss: 2774.4835401088467, K-means Loss: 330.2423095703125\n",
      "Epoch 905\n",
      "Reconstruction Loss: 2775.956071605917, K-means Loss: 328.39251708984375\n",
      "Epoch 906\n",
      "Reconstruction Loss: 2772.8095087299407, K-means Loss: 331.1636657714844\n",
      "Epoch 907\n",
      "Reconstruction Loss: 2774.736227694822, K-means Loss: 328.9455261230469\n",
      "Epoch 908\n",
      "Reconstruction Loss: 2771.175513807329, K-means Loss: 332.2886047363281\n",
      "Epoch 909\n",
      "Reconstruction Loss: 2773.6877152173283, K-means Loss: 329.7966003417969\n",
      "Epoch 910\n",
      "Reconstruction Loss: 2769.7201025755107, K-means Loss: 333.9418029785156\n",
      "Epoch 911\n",
      "Reconstruction Loss: 2772.9315080812034, K-means Loss: 331.08154296875\n",
      "Epoch 912\n",
      "Reconstruction Loss: 2768.4666102595334, K-means Loss: 336.07452392578125\n",
      "Epoch 913\n",
      "Reconstruction Loss: 2772.5083570412544, K-means Loss: 332.8974304199219\n",
      "Epoch 914\n",
      "Reconstruction Loss: 2767.5307844862473, K-means Loss: 338.7832336425781\n",
      "Epoch 915\n",
      "Reconstruction Loss: 2772.148486995377, K-means Loss: 335.1750793457031\n",
      "Epoch 916\n",
      "Reconstruction Loss: 2766.582105600016, K-means Loss: 341.6160583496094\n",
      "Epoch 917\n",
      "Reconstruction Loss: 2771.6458292650427, K-means Loss: 337.40283203125\n",
      "Epoch 918\n",
      "Reconstruction Loss: 2765.3463764727067, K-means Loss: 343.80999755859375\n",
      "Epoch 919\n",
      "Reconstruction Loss: 2770.0687238467794, K-means Loss: 338.4391784667969\n",
      "Epoch 920\n",
      "Reconstruction Loss: 2763.209426338215, K-means Loss: 343.81976318359375\n",
      "Epoch 921\n",
      "Reconstruction Loss: 2767.207874881253, K-means Loss: 337.5205383300781\n",
      "Epoch 922\n",
      "Reconstruction Loss: 2760.4268333429536, K-means Loss: 341.5750732421875\n",
      "Epoch 923\n",
      "Reconstruction Loss: 2763.8268216413685, K-means Loss: 335.23089599609375\n",
      "Epoch 924\n",
      "Reconstruction Loss: 2757.635858577188, K-means Loss: 338.36126708984375\n",
      "Epoch 925\n",
      "Reconstruction Loss: 2760.510997762862, K-means Loss: 332.5818176269531\n",
      "Epoch 926\n",
      "Reconstruction Loss: 2755.1278544611027, K-means Loss: 335.1488342285156\n",
      "Epoch 927\n",
      "Reconstruction Loss: 2757.417456664266, K-means Loss: 330.3607482910156\n",
      "Epoch 928\n",
      "Reconstruction Loss: 2752.8138734551153, K-means Loss: 332.6259460449219\n",
      "Epoch 929\n",
      "Reconstruction Loss: 2754.6061626573824, K-means Loss: 328.7176818847656\n",
      "Epoch 930\n",
      "Reconstruction Loss: 2750.6371842692483, K-means Loss: 330.7742919921875\n",
      "Epoch 931\n",
      "Reconstruction Loss: 2752.0261839439777, K-means Loss: 327.6925964355469\n",
      "Epoch 932\n",
      "Reconstruction Loss: 2748.701538919971, K-means Loss: 329.43853759765625\n",
      "Epoch 933\n",
      "Reconstruction Loss: 2749.7203708577226, K-means Loss: 327.016845703125\n",
      "Epoch 934\n",
      "Reconstruction Loss: 2746.9095969484056, K-means Loss: 328.5018615722656\n",
      "Epoch 935\n",
      "Reconstruction Loss: 2747.657245756345, K-means Loss: 326.5945739746094\n",
      "Epoch 936\n",
      "Reconstruction Loss: 2745.2131583908094, K-means Loss: 327.91522216796875\n",
      "Epoch 937\n",
      "Reconstruction Loss: 2745.676941481051, K-means Loss: 326.4256591796875\n",
      "Epoch 938\n",
      "Reconstruction Loss: 2743.451892982267, K-means Loss: 327.6404113769531\n",
      "Epoch 939\n",
      "Reconstruction Loss: 2743.6916858339077, K-means Loss: 326.4494934082031\n",
      "Epoch 940\n",
      "Reconstruction Loss: 2741.7038411447134, K-means Loss: 327.5022277832031\n",
      "Epoch 941\n",
      "Reconstruction Loss: 2741.7578103300075, K-means Loss: 326.5703430175781\n",
      "Epoch 942\n",
      "Reconstruction Loss: 2740.019824379029, K-means Loss: 327.4644775390625\n",
      "Epoch 943\n",
      "Reconstruction Loss: 2739.9632375027736, K-means Loss: 326.705078125\n",
      "Epoch 944\n",
      "Reconstruction Loss: 2738.3556905395694, K-means Loss: 327.5262145996094\n",
      "Epoch 945\n",
      "Reconstruction Loss: 2738.231692382397, K-means Loss: 326.87548828125\n",
      "Epoch 946\n",
      "Reconstruction Loss: 2736.722633503521, K-means Loss: 327.64453125\n",
      "Epoch 947\n",
      "Reconstruction Loss: 2736.4875964180433, K-means Loss: 327.1416015625\n",
      "Epoch 948\n",
      "Reconstruction Loss: 2735.09751779888, K-means Loss: 327.8348388671875\n",
      "Epoch 949\n",
      "Reconstruction Loss: 2734.746420444705, K-means Loss: 327.4758605957031\n",
      "Epoch 950\n",
      "Reconstruction Loss: 2733.499551632947, K-means Loss: 328.0832214355469\n",
      "Epoch 951\n",
      "Reconstruction Loss: 2733.060286125937, K-means Loss: 327.8777160644531\n",
      "Epoch 952\n",
      "Reconstruction Loss: 2731.9784270103605, K-means Loss: 328.3895568847656\n",
      "Epoch 953\n",
      "Reconstruction Loss: 2731.3902939101017, K-means Loss: 328.3497619628906\n",
      "Epoch 954\n",
      "Reconstruction Loss: 2730.4022744461545, K-means Loss: 328.77239990234375\n",
      "Epoch 955\n",
      "Reconstruction Loss: 2729.73171178975, K-means Loss: 328.83294677734375\n",
      "Epoch 956\n",
      "Reconstruction Loss: 2728.8420334939724, K-means Loss: 329.18438720703125\n",
      "Epoch 957\n",
      "Reconstruction Loss: 2728.0759871225628, K-means Loss: 329.3631286621094\n",
      "Epoch 958\n",
      "Reconstruction Loss: 2727.310714286159, K-means Loss: 329.5628967285156\n",
      "Epoch 959\n",
      "Reconstruction Loss: 2726.3894575907816, K-means Loss: 329.8349609375\n",
      "Epoch 960\n",
      "Reconstruction Loss: 2725.7419695324147, K-means Loss: 329.8044128417969\n",
      "Epoch 961\n",
      "Reconstruction Loss: 2724.6317156692144, K-means Loss: 330.1313781738281\n",
      "Epoch 962\n",
      "Reconstruction Loss: 2724.103720839728, K-means Loss: 329.8035583496094\n",
      "Epoch 963\n",
      "Reconstruction Loss: 2722.856177139294, K-means Loss: 330.1706848144531\n",
      "Epoch 964\n",
      "Reconstruction Loss: 2722.45756335992, K-means Loss: 329.6221618652344\n",
      "Epoch 965\n",
      "Reconstruction Loss: 2721.0561587999455, K-means Loss: 330.03582763671875\n",
      "Epoch 966\n",
      "Reconstruction Loss: 2720.7141691200786, K-means Loss: 329.3021240234375\n",
      "Epoch 967\n",
      "Reconstruction Loss: 2719.148317086623, K-means Loss: 329.78961181640625\n",
      "Epoch 968\n",
      "Reconstruction Loss: 2718.9343672196455, K-means Loss: 328.8935852050781\n",
      "Epoch 969\n",
      "Reconstruction Loss: 2717.214176810509, K-means Loss: 329.5281066894531\n",
      "Epoch 970\n",
      "Reconstruction Loss: 2717.2084752063383, K-means Loss: 328.4691467285156\n",
      "Epoch 971\n",
      "Reconstruction Loss: 2715.3222144303763, K-means Loss: 329.3306579589844\n",
      "Epoch 972\n",
      "Reconstruction Loss: 2715.598093140924, K-means Loss: 328.0889892578125\n",
      "Epoch 973\n",
      "Reconstruction Loss: 2713.4654228851314, K-means Loss: 329.25732421875\n",
      "Epoch 974\n",
      "Reconstruction Loss: 2714.11582136554, K-means Loss: 327.78131103515625\n",
      "Epoch 975\n",
      "Reconstruction Loss: 2711.6938852947737, K-means Loss: 329.3824462890625\n",
      "Epoch 976\n",
      "Reconstruction Loss: 2712.7451184842007, K-means Loss: 327.6414489746094\n",
      "Epoch 977\n",
      "Reconstruction Loss: 2710.0087648533067, K-means Loss: 329.7296142578125\n",
      "Epoch 978\n",
      "Reconstruction Loss: 2711.548578345568, K-means Loss: 327.7566223144531\n",
      "Epoch 979\n",
      "Reconstruction Loss: 2708.4321541982517, K-means Loss: 330.67047119140625\n",
      "Epoch 980\n",
      "Reconstruction Loss: 2710.8059234142856, K-means Loss: 328.3849792480469\n",
      "Epoch 981\n",
      "Reconstruction Loss: 2707.160024934859, K-means Loss: 332.5076599121094\n",
      "Epoch 982\n",
      "Reconstruction Loss: 2710.599591554361, K-means Loss: 329.8377380371094\n",
      "Epoch 983\n",
      "Reconstruction Loss: 2706.3708894706288, K-means Loss: 335.4045104980469\n",
      "Epoch 984\n",
      "Reconstruction Loss: 2711.008998979927, K-means Loss: 332.2996826171875\n",
      "Epoch 985\n",
      "Reconstruction Loss: 2705.959601885371, K-means Loss: 339.2984619140625\n",
      "Epoch 986\n",
      "Reconstruction Loss: 2711.68604842555, K-means Loss: 335.388427734375\n",
      "Epoch 987\n",
      "Reconstruction Loss: 2705.4540118562977, K-means Loss: 343.31854248046875\n",
      "Epoch 988\n",
      "Reconstruction Loss: 2711.798386967627, K-means Loss: 338.2622985839844\n",
      "Epoch 989\n",
      "Reconstruction Loss: 2704.5242964098793, K-means Loss: 346.21771240234375\n",
      "Epoch 990\n",
      "Reconstruction Loss: 2710.7345207308313, K-means Loss: 339.94580078125\n",
      "Epoch 991\n",
      "Reconstruction Loss: 2702.843431608903, K-means Loss: 347.043701171875\n",
      "Epoch 992\n",
      "Reconstruction Loss: 2708.3682231210973, K-means Loss: 339.9705810546875\n",
      "Epoch 993\n",
      "Reconstruction Loss: 2700.462398089419, K-means Loss: 345.8495178222656\n",
      "Epoch 994\n",
      "Reconstruction Loss: 2705.157973702138, K-means Loss: 338.576904296875\n",
      "Epoch 995\n",
      "Reconstruction Loss: 2697.662787046111, K-means Loss: 343.2919921875\n",
      "Epoch 996\n",
      "Reconstruction Loss: 2701.469511675897, K-means Loss: 336.30487060546875\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 997\n",
      "Reconstruction Loss: 2694.8921913752174, K-means Loss: 339.8724365234375\n",
      "Epoch 998\n",
      "Reconstruction Loss: 2698.0573215977847, K-means Loss: 333.9107971191406\n",
      "Epoch 999\n",
      "Reconstruction Loss: 2692.514554509056, K-means Loss: 336.900146484375\n"
     ]
    }
   ],
   "source": [
    "recon_losses = []\n",
    "kmeans_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    x_torch = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "    y_torch = torch.from_numpy(X_train).type(torch.FloatTensor)\n",
    "\n",
    "    x = Variable(x_torch)\n",
    "    y = Variable(y_torch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "#     k_means_loss, _, outputs = model(x)\n",
    "    output_tuple = model(x)\n",
    "    cost = criterion(output_tuple.output, y) + 0.00001 * output_tuple.k_means_loss\n",
    "#     cost = criterion(outputs, y) + 0.00001 * k_means_loss\n",
    "    cost.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    recon_loss = float(((y_torch - output_tuple.output.data)**2).mean())\n",
    "    kmeans_loss = float(0.00001 * output_tuple.k_means_loss)\n",
    "    print(\"Epoch {}\".format(epoch))\n",
    "    print(\"Reconstruction Loss: {0}, K-means Loss: {1}\".format(recon_loss, kmeans_loss))\n",
    "    recon_losses.append(recon_loss)\n",
    "    kmeans_losses.append(kmeans_loss)\n",
    "#     print(((y_torch - outputs.data)**2).mean())\n",
    "#     print(0.00001 * k_means_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_hidden, _ = model(Variable(torch.from_numpy(X_train.values).type(torch.FloatTensor)))\n",
    "# X_test_hidden, _ = model(Variable(torch.from_numpy(X_test.values).type(torch.FloatTensor)))\n",
    "\n",
    "output_tuple_train = model(Variable(torch.from_numpy(X_train).type(torch.FloatTensor)))\n",
    "output_tuple_test = model(Variable(torch.from_numpy(X_test).type(torch.FloatTensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-4.9652e-01  8.7988e-01 -3.6134e-01  ...   1.3048e+00  1.1890e+00  1.0158e+00\n",
       " 1.0322e+00  2.0356e+00  4.6983e-01  ...   1.4306e+00  2.0928e+00  1.1317e+00\n",
       "-9.6654e-02  1.7831e+00  3.6032e-01  ...   1.1337e+00 -3.4721e-01  7.4321e-01\n",
       "                ...                                      ...                \n",
       "-1.8232e+00 -1.0643e+00  1.4060e+00  ...  -4.3853e-01 -4.8646e-01  3.7386e-01\n",
       "-3.6603e-01 -5.4236e-01  1.5061e+00  ...   4.1809e-01 -4.3590e-01  1.2563e+00\n",
       "-2.3730e+00  9.7460e-01 -1.1650e+00  ...  -2.5269e+00 -1.5336e+00 -1.0853e+00\n",
       "[torch.FloatTensor of size 10x784]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tuple_train.output[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Variable containing:\n",
       "  8\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  3\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  9\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  4\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  6\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  7\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  4\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  1\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  4\n",
       " [torch.LongTensor of size 1], Variable containing:\n",
       "  6\n",
       " [torch.LongTensor of size 1]]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_tuple_train.clusters[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calvinku/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8317708333333333\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/calvinku/anaconda3/envs/python3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "X_train_hidden = X_train_hidden.data.numpy()\n",
    "X_test_hidden = X_test_hidden.data.numpy()\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train_hidden, y_train)\n",
    "y_pred = lr.predict_proba(X_test_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8052083333333334\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(y_test, y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(1), requires_grad=True)\n",
    "y = 5 * (x + 1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 20\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = 0.5 * torch.sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 10\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 10\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48660327761141575"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.rand(100).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "centers = torch.from_numpy(np.random.rand(10, 784)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = torch.from_numpy(np.random.rand(20000, 784)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([20000, 784])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "126.69114151000977"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((dataset[0] - centers)**2).sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\n",
       "  135.5187\n",
       " [torch.FloatTensor of size 1], \n",
       "  2\n",
       " [torch.LongTensor of size 1])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((dataset[0] - centers)**2).sum(1).max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 0, 3, 3, 6, 4, 4, 0, 4, 9]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[((data - centers)**2).sum(1).min(0)[1][0] for data in dataset][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       "  119.5014\n",
       "  123.2441\n",
       "  135.5187\n",
       "  129.4469\n",
       "  117.6147\n",
       "  128.2050\n",
       "  122.8010\n",
       "  129.3588\n",
       "  126.3594\n",
       "  134.8614\n",
       " [torch.FloatTensor of size 10], \n",
       "  125.4285\n",
       "  130.5832\n",
       "  135.6001\n",
       "  138.1736\n",
       "  135.7134\n",
       "  129.4753\n",
       "  133.7235\n",
       "  141.2342\n",
       "  144.9864\n",
       "  134.1505\n",
       " [torch.FloatTensor of size 10], \n",
       "  133.7800\n",
       "  128.7815\n",
       "  131.9692\n",
       "  122.2993\n",
       "  129.3626\n",
       "  133.6053\n",
       "  130.1130\n",
       "  132.8036\n",
       "  134.6575\n",
       "  125.4915\n",
       " [torch.FloatTensor of size 10], \n",
       "  131.9848\n",
       "  131.0141\n",
       "  134.8151\n",
       "  125.2208\n",
       "  128.1747\n",
       "  132.2542\n",
       "  136.5810\n",
       "  137.9877\n",
       "  132.6785\n",
       "  127.1587\n",
       " [torch.FloatTensor of size 10], \n",
       "  137.0968\n",
       "  132.0614\n",
       "  129.9968\n",
       "  133.9629\n",
       "  135.1674\n",
       "  135.2247\n",
       "  125.4944\n",
       "  128.4681\n",
       "  134.7811\n",
       "  143.6792\n",
       " [torch.FloatTensor of size 10], \n",
       "  127.8875\n",
       "  122.6231\n",
       "  127.5521\n",
       "  125.7077\n",
       "  118.4951\n",
       "  125.3392\n",
       "  132.6665\n",
       "  123.1378\n",
       "  127.9611\n",
       "  131.0582\n",
       " [torch.FloatTensor of size 10], \n",
       "  135.2402\n",
       "  136.2485\n",
       "  135.2043\n",
       "  130.9381\n",
       "  125.1189\n",
       "  134.3459\n",
       "  155.2023\n",
       "  127.7248\n",
       "  138.9448\n",
       "  131.5292\n",
       " [torch.FloatTensor of size 10], \n",
       "  123.6201\n",
       "  134.0613\n",
       "  126.8304\n",
       "  127.3502\n",
       "  125.4206\n",
       "  127.2046\n",
       "  136.7286\n",
       "  136.8343\n",
       "  130.0554\n",
       "  134.3181\n",
       " [torch.FloatTensor of size 10], \n",
       "  134.2224\n",
       "  128.9292\n",
       "  140.2548\n",
       "  140.2462\n",
       "  126.5685\n",
       "  134.5458\n",
       "  129.0803\n",
       "  135.0546\n",
       "  134.2522\n",
       "  130.0306\n",
       " [torch.FloatTensor of size 10], \n",
       "  137.2277\n",
       "  132.4750\n",
       "  129.0275\n",
       "  130.2563\n",
       "  124.3763\n",
       "  124.4687\n",
       "  133.6750\n",
       "  134.9368\n",
       "  125.2946\n",
       "  123.4692\n",
       " [torch.FloatTensor of size 10]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "([((data - centers)**2).sum(1) for data in dataset][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0329911708831787"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v1 = torch.from_numpy(np.random.rand(1, 15)).type(torch.FloatTensor)\n",
    "v2 = torch.from_numpy(np.random.rand(1, 15)).type(torch.FloatTensor)\n",
    "\n",
    "def square_loss(v1, v2):\n",
    "    loss = ((v1 - v2)**2).sum(1)[0]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "square_loss(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

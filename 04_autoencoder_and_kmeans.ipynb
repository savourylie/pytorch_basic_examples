{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mnist\n",
    "\n",
    "# data_folder  = '../../data/mnist/'\n",
    "data_folder = '/Users/calvinku/Dropbox/project_dstk/dstk_data/mnist/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from glob import glob\n",
    "# import os\n",
    "\n",
    "data = pd.read_csv(data_folder + 'train.csv')\n",
    "\n",
    "# mnist.init()\n",
    "# X_train, y_train, X_test, y_test = mnist.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features, label = data.drop(['label'], axis=1), data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, label, test_size=0.3, random_state=1113)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.4, random_state=1113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((17640, 784), (12600, 784), (17640,), (12600,))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train_mnist.pkl', 'wb') as handle:\n",
    "    pickle.dump(X_train, handle)\n",
    "with open('X_test_mnist.pkl', 'wb') as handle:\n",
    "    pickle.dump(X_test, handle)\n",
    "with open('y_train_mnist.pkl', 'wb') as handle:\n",
    "    pickle.dump(y_train, handle)\n",
    "with open('y_test_mnist.pkl', 'wb') as handle:\n",
    "    pickle.dump(y_test, handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('X_train_mnist.pkl', 'rb') as handle:\n",
    "    X_train = pickle.load(handle)\n",
    "with open('X_test_mnist.pkl', 'rb') as handle:\n",
    "    X_test = pickle.load(handle)\n",
    "with open('y_train_mnist.pkl', 'rb') as handle:\n",
    "    y_train = pickle.load(handle)\n",
    "with open('y_test_mnist.pkl', 'rb') as handle:\n",
    "    y_test = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "\n",
    "class AutoencoderKmeans(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim=10, num_clusters=10):\n",
    "        super().__init__()\n",
    "        # Global timestamp\n",
    "        self.t = 0\n",
    "        \n",
    "        # Hyper parameters\n",
    "        self.num_clusters = num_clusters\n",
    "        self.encoding_dim = encoding_dim\n",
    "        \n",
    "        # Define AE architecture\n",
    "#         self.fc1 = nn.Linear(input_dim, self.encoding_dim)\n",
    "#         self.fc2 = nn.Linear(self.encoding_dim, input_dim)\n",
    "        self.enc1 = nn.Linear(input_dim, 1000)\n",
    "        self.enc2 = nn.Linear(1000, 250)\n",
    "        self.enc3 = nn.Linear(250, 50)\n",
    "        self.enc4 = nn.Linear(50, self.encoding_dim)\n",
    "        self.dec1 = nn.Linear(self.encoding_dim, 50)\n",
    "        self.dec2 = nn.Linear(50, 250)\n",
    "        self.dec3 = nn.Linear(250, 1000)\n",
    "        self.dec4 = nn.Linear(1000, input_dim)\n",
    "\n",
    "        # Initialize cluster centers\n",
    "        self.centers = torch.from_numpy(np.random.uniform(0, 1, (self.num_clusters, self.encoding_dim))).type(torch.FloatTensor)\n",
    "        self.clusters = None\n",
    "        self.AKoutput = namedtuple('AKoutput', 'k_means_loss clusters data_hidden output')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.training:\n",
    "#             x = self.fc1(x)\n",
    "\n",
    "#             data_hidden = x.clone()\n",
    "#             clusters = self.get_clusters(data_hidden)\n",
    "#             k_means_loss = self.get_kmeans_loss(data_hidden, clusters)\n",
    "#             self.centers = self.update_centers(data_hidden, clusters)\n",
    "\n",
    "#             x = self.fc2(x)\n",
    "\n",
    "#             output = x\n",
    "\n",
    "#             self.t += 1\n",
    "\n",
    "#             output_tuple = self.AKoutput(k_means_loss=k_means_loss, clusters=clusters, data_hidden=data_hidden, output=output)\n",
    "#     #         return k_means_loss, clusters, data_hidden, output\n",
    "#             return output_tuple\n",
    "            x = self.enc1(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.enc2(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.enc3(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.enc4(x)\n",
    "            \n",
    "            data_hidden = x.clone()\n",
    "            clusters = self.get_clusters(data_hidden)\n",
    "            k_means_loss = self.get_kmeans_loss(data_hidden, clusters)\n",
    "            self.centers = self.update_centers(data_hidden, clusters)\n",
    "            \n",
    "            x = self.dec1(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.dec2(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.dec3(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.dec4(x)\n",
    "            \n",
    "            output = x\n",
    "\n",
    "            self.t += 1\n",
    "\n",
    "            output_tuple = self.AKoutput(k_means_loss=k_means_loss, clusters=clusters, data_hidden=data_hidden, output=output)\n",
    "            return output_tuple\n",
    "    \n",
    "        else:\n",
    "#             x = self.fc1(x)\n",
    "#             data_hidden = x.clone()\n",
    "#             clusters = self.get_clusters(data_hidden)\n",
    "#             k_means_loss = self.get_kmeans_loss(data_hidden, clusters)\n",
    "#             x = self.fc2(x)\n",
    "#             output = x\n",
    "#             self.t += 1\n",
    "#             output_tuple = self.AKoutput(k_means_loss=k_means_loss, clusters=clusters, data_hidden=data_hidden, output=output)\n",
    "#             return output_tuple\n",
    "            x = self.enc1(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.enc2(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.enc3(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.enc4(x)\n",
    "            \n",
    "            data_hidden = x.clone()\n",
    "            clusters = self.get_clusters(data_hidden)\n",
    "            k_means_loss = self.get_kmeans_loss(data_hidden, clusters)\n",
    "            \n",
    "            x = self.dec1(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.dec2(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.dec3(x)\n",
    "            x = F.tanh(x)\n",
    "            x = self.dec4(x)\n",
    "        \n",
    "            output = x\n",
    "            self.t += 1\n",
    "            output_tuple = self.AKoutput(k_means_loss=k_means_loss, clusters=clusters, data_hidden=data_hidden, output=output)\n",
    "            return output_tuple\n",
    "            \n",
    "    def get_clusters(self, dataset):\n",
    "        clusters = []\n",
    "        if isinstance(self.centers, torch.autograd.Variable):\n",
    "            self.centers = self.centers.data\n",
    "        for data in dataset:\n",
    "            clusters.append(((data - Variable(self.centers, requires_grad=False))**2).sum(1).min(0)[1][0])\n",
    "        return clusters\n",
    "    \n",
    "    def get_kmeans_loss(self, dataset, clusters):\n",
    "        loss = [0 for i in range(self.num_clusters)]\n",
    "        \n",
    "        for i, cluster in enumerate(clusters):\n",
    "            loss[int(cluster.data)] += ((dataset[i] - Variable(self.centers[int(cluster.data)], requires_grad=False))**2).sum()\n",
    "            \n",
    "        return sum(loss) / len(loss)\n",
    "    \n",
    "    def update_centers(self, dataset, clusters):\n",
    "        data_by_cluster = [[] for i in range(self.num_clusters)]\n",
    "        \n",
    "        for i, x in enumerate(clusters):\n",
    "            data_by_cluster[int(x.data)].append(dataset[i])\n",
    "        \n",
    "        try:\n",
    "            centers = [sum(data) / len(data) for data in data_by_cluster]\n",
    "        except ZeroDivisionError:\n",
    "            centers = torch.from_numpy(np.random.uniform(0, 1, (self.num_clusters, self.encoding_dim))).type(torch.FloatTensor)\n",
    "            print(\"Cluster centers reinitialized.\")\n",
    "        else:\n",
    "            centers = torch.stack(centers)\n",
    "        \n",
    "        return centers\n",
    "        \n",
    "    @staticmethod\n",
    "    def __square_loss(v1, v2):\n",
    "        loss = ((v1 - v2)**2).sum(1)[0]\n",
    "        \n",
    "        return loss        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_dim = 784\n",
    "\n",
    "model = AutoencoderKmeans(input_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1000, 784])\n",
      "torch.Size([1000])\n",
      "torch.Size([250, 1000])\n",
      "torch.Size([250])\n",
      "torch.Size([50, 250])\n",
      "torch.Size([50])\n",
      "torch.Size([10, 50])\n",
      "torch.Size([10])\n",
      "torch.Size([50, 10])\n",
      "torch.Size([50])\n",
      "torch.Size([250, 50])\n",
      "torch.Size([250])\n",
      "torch.Size([1000, 250])\n",
      "torch.Size([1000])\n",
      "torch.Size([784, 1000])\n",
      "torch.Size([784])\n"
     ]
    }
   ],
   "source": [
    "num_layers = len(list(model.parameters()))\n",
    "\n",
    "for i in range(num_layers):\n",
    "    print(list(model.parameters())[i].size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "learning_rate = 0.003\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "lambd = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Reconstruction Loss: 7318.369548296176, K-means Loss: 2.6832618713378906\n",
      "Validation: R-Loss: 7251.246425469867, K-means Loss: 9.43791675567627\n",
      "Epoch 10\n",
      "Reconstruction Loss: 5970.422578617838, K-means Loss: 58.947303771972656\n",
      "Validation: R-Loss: 5805.99590705002, K-means Loss: 45.95074462890625\n",
      "Epoch 20\n",
      "Reconstruction Loss: 4993.699474278352, K-means Loss: 81.84081268310547\n",
      "Validation: R-Loss: 4898.60415401716, K-means Loss: 60.782920837402344\n",
      "Epoch 30\n",
      "Reconstruction Loss: 4559.550922255236, K-means Loss: 106.4798355102539\n",
      "Validation: R-Loss: 4509.717878311879, K-means Loss: 76.45944213867188\n",
      "Epoch 40\n",
      "Reconstruction Loss: 4425.379498230534, K-means Loss: 104.82664489746094\n",
      "Validation: R-Loss: 4394.613950887665, K-means Loss: 77.61102294921875\n",
      "Epoch 50\n",
      "Reconstruction Loss: 4397.524989394506, K-means Loss: 113.53317260742188\n",
      "Validation: R-Loss: 4371.735995865872, K-means Loss: 76.66045379638672\n",
      "Epoch 60\n",
      "Reconstruction Loss: 4392.056139297693, K-means Loss: 120.85234069824219\n",
      "Validation: R-Loss: 4367.0823111616755, K-means Loss: 77.38219451904297\n",
      "Epoch 70\n",
      "Reconstruction Loss: 4389.852199327239, K-means Loss: 116.80610656738281\n",
      "Validation: R-Loss: 4364.874564791985, K-means Loss: 75.99473571777344\n",
      "Epoch 80\n",
      "Reconstruction Loss: 4388.575067743737, K-means Loss: 118.33523559570312\n",
      "Validation: R-Loss: 4363.534404319879, K-means Loss: 79.6688003540039\n",
      "Epoch 90\n",
      "Reconstruction Loss: 4387.979459957323, K-means Loss: 114.78223419189453\n",
      "Validation: R-Loss: 4362.877076398317, K-means Loss: 78.55260467529297\n"
     ]
    }
   ],
   "source": [
    "recon_losses = []\n",
    "kmeans_losses = []\n",
    "# kmeans_lambda = 0.001\n",
    "kmeans_lambda = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    x_torch = torch.from_numpy(X_train.values).type(torch.FloatTensor)\n",
    "    y_torch = torch.from_numpy(X_train.values).type(torch.FloatTensor)\n",
    "\n",
    "    x = Variable(x_torch)\n",
    "    y = Variable(y_torch)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    model.train()\n",
    "    output_tuple = model(x)\n",
    "    cost = criterion(output_tuple.output, y) + kmeans_lambda * output_tuple.k_means_loss\n",
    "    cost.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "    \n",
    "    recon_loss = float(((y_torch - output_tuple.output.data)**2).mean())\n",
    "    kmeans_loss = float(0.001 * output_tuple.k_means_loss)\n",
    "    recon_losses.append(recon_loss)\n",
    "    kmeans_losses.append(kmeans_loss)\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch {}\".format(epoch))\n",
    "        print(\"Reconstruction Loss: {0}, K-means Loss: {1}\".format(recon_loss, kmeans_loss))\n",
    "        \n",
    "        x_torch_val = torch.from_numpy(X_val.values).type(torch.FloatTensor)\n",
    "        y_torch_val = torch.from_numpy(X_val.values).type(torch.FloatTensor)\n",
    "        x_val = Variable(x_torch_val)\n",
    "        y_val = Variable(x_torch_val)\n",
    "        model.eval()\n",
    "        output_tuple_val = model(x_val)\n",
    "        recon_loss_val = float(((y_torch_val - output_tuple_val.output.data)**2).mean())\n",
    "        kmeans_loss_val = float(0.001 * output_tuple_val.k_means_loss)\n",
    "        print(\"Validation: R-Loss: {0}, K-means Loss: {1}\".format(recon_loss_val, kmeans_loss_val))\n",
    "#     print(((y_torch - outputs.data)**2).mean())\n",
    "#     print(0.00001 * k_means_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# X_train_hidden, _ = model(Variable(torch.from_numpy(X_train.values).type(torch.FloatTensor)))\n",
    "# X_test_hidden, _ = model(Variable(torch.from_numpy(X_test.values).type(torch.FloatTensor)))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "output_tuple_train = model(Variable(torch.from_numpy(X_train.values).type(torch.FloatTensor)))\n",
    "output_tuple_test = model(Variable(torch.from_numpy(X_test.values).type(torch.FloatTensor)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "\n",
    "# y_train.shape, len([x for x in X_train.values])\n",
    "normalized_mutual_info_score(y_train, [int(x) for x in output_tuple_train.clusters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_reshape = output_tuple_train.output.data.numpy().reshape(17640, 28, 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_reshape[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "plt.imshow(output_reshape[100], interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(roc_auc_score(y_test, y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_hidden = X_train_hidden.data.numpy()\n",
    "X_test_hidden = X_test_hidden.data.numpy()\n",
    "\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(X_train_hidden, y_train)\n",
    "y_pred = lr.predict_proba(X_test_hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(roc_auc_score(y_test, y_pred[:,1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(1), requires_grad=True)\n",
    "y = 5 * (x + 1) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o = 0.5 * torch.sum(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.rand(100).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers = torch.from_numpy(np.random.rand(10, 784)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset = torch.from_numpy(np.random.rand(20000, 784)).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((dataset[0] - centers)**2).sum(1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((dataset[0] - centers)**2).sum(1).max(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[((data - centers)**2).sum(1).min(0)[1][0] for data in dataset][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "([((data - centers)**2).sum(1) for data in dataset][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = torch.from_numpy(np.random.rand(1, 15)).type(torch.FloatTensor)\n",
    "v2 = torch.from_numpy(np.random.rand(1, 15)).type(torch.FloatTensor)\n",
    "\n",
    "def square_loss(v1, v2):\n",
    "    loss = ((v1 - v2)**2).sum(1)[0]\n",
    "    \n",
    "    return loss\n",
    "\n",
    "square_loss(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pytorch]",
   "language": "python",
   "name": "conda-env-pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
